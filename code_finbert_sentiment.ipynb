{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Apple Set To Beat Q1 Earnings Estimates  Tech ...</td>\n",
       "      <td>Technology giant Apple   NASDAQ AAPL   is set ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Tech Daily  Intel Results  Netflix Surge  Appl...</td>\n",
       "      <td>The top stories in this digest are Intel s   N...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>7 Monster Stock Market Predictions For The Wee...</td>\n",
       "      <td>S P 500  SPY \\nThis week will be packed with e...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Apple Earnings Preview  5G Launch  Expanding S...</td>\n",
       "      <td>Reports Q1 2020 results on Tuesday  Jan  28 ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Buy Surging Apple   Microsoft Stock Before Qua...</td>\n",
       "      <td>On today s episode of Full Court Finance here ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 ticker        Date category  \\\n",
       "0           0   AAPL  2020-01-27  opinion   \n",
       "1           1   AAPL  2020-01-27  opinion   \n",
       "2           2   AAPL  2020-01-27  opinion   \n",
       "3           3   AAPL  2020-01-27  opinion   \n",
       "4           4   AAPL  2020-01-27  opinion   \n",
       "\n",
       "                                               title  \\\n",
       "0  Apple Set To Beat Q1 Earnings Estimates  Tech ...   \n",
       "1  Tech Daily  Intel Results  Netflix Surge  Appl...   \n",
       "2  7 Monster Stock Market Predictions For The Wee...   \n",
       "3  Apple Earnings Preview  5G Launch  Expanding S...   \n",
       "4  Buy Surging Apple   Microsoft Stock Before Qua...   \n",
       "\n",
       "                                             content       Open       High  \\\n",
       "0  Technology giant Apple   NASDAQ AAPL   is set ...  77.514999  77.942497   \n",
       "1  The top stories in this digest are Intel s   N...  77.514999  77.942497   \n",
       "2  S P 500  SPY \\nThis week will be packed with e...  77.514999  77.942497   \n",
       "3    Reports Q1 2020 results on Tuesday  Jan  28 ...  77.514999  77.942497   \n",
       "4  On today s episode of Full Court Finance here ...  77.514999  77.942497   \n",
       "\n",
       "         Low      Close  Adj Close     Volume  label  \n",
       "0  76.220001  77.237503  75.793358  161940000      0  \n",
       "1  76.220001  77.237503  75.793358  161940000      0  \n",
       "2  76.220001  77.237503  75.793358  161940000      0  \n",
       "3  76.220001  77.237503  75.793358  161940000      0  \n",
       "4  76.220001  77.237503  75.793358  161940000      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './data/NEWS_YAHOO_stock_prediction.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Technology giant Apple   NASDAQ AAPL   is set ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>The top stories in this digest are Intel s   N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>S P 500  SPY \\nThis week will be packed with e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Reports Q1 2020 results on Tuesday  Jan  28 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>On today s episode of Full Court Finance here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Monday  January 27  2020The Zacks Research Dai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>news</td>\n",
       "      <td>By Peter Nurse \\nInvesting com   European stoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>news</td>\n",
       "      <td>BTIG analyst Mark Palmer initiates coverage of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>opinion</td>\n",
       "      <td>I got a great question recently from Barbara P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>opinion</td>\n",
       "      <td>We get into the heart of the Q4 earnings seaso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date category                                            content\n",
       "0  2020-01-27  opinion  Technology giant Apple   NASDAQ AAPL   is set ...\n",
       "1  2020-01-27  opinion  The top stories in this digest are Intel s   N...\n",
       "2  2020-01-27  opinion  S P 500  SPY \\nThis week will be packed with e...\n",
       "3  2020-01-27  opinion    Reports Q1 2020 results on Tuesday  Jan  28 ...\n",
       "4  2020-01-27  opinion  On today s episode of Full Court Finance here ...\n",
       "5  2020-01-27  opinion  Monday  January 27  2020The Zacks Research Dai...\n",
       "6  2020-01-27     news  By Peter Nurse \\nInvesting com   European stoc...\n",
       "7  2020-01-27     news  BTIG analyst Mark Palmer initiates coverage of...\n",
       "8  2020-01-24  opinion  I got a great question recently from Barbara P...\n",
       "9  2020-01-24  opinion  We get into the heart of the Q4 earnings seaso..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Date', 'category', 'content']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15965 entries, 0 to 15974\n",
      "Data columns (total 12 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ticker     15965 non-null  object \n",
      " 1   Date       15965 non-null  object \n",
      " 2   category   15965 non-null  object \n",
      " 3   title      15965 non-null  object \n",
      " 4   content    15965 non-null  object \n",
      " 5   Open       15965 non-null  float64\n",
      " 6   High       15965 non-null  float64\n",
      " 7   Low        15965 non-null  float64\n",
      " 8   Close      15965 non-null  float64\n",
      " 9   Adj Close  15965 non-null  float64\n",
      " 10  Volume     15965 non-null  int64  \n",
      " 11  label      15965 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>1.596500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.583061</td>\n",
       "      <td>40.952148</td>\n",
       "      <td>40.241173</td>\n",
       "      <td>40.605005</td>\n",
       "      <td>38.739098</td>\n",
       "      <td>1.536463e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.884583</td>\n",
       "      <td>11.980327</td>\n",
       "      <td>11.799389</td>\n",
       "      <td>11.891820</td>\n",
       "      <td>12.158320</td>\n",
       "      <td>1.096033e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.856071</td>\n",
       "      <td>14.271429</td>\n",
       "      <td>13.753571</td>\n",
       "      <td>13.947500</td>\n",
       "      <td>12.084597</td>\n",
       "      <td>4.544800e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.522499</td>\n",
       "      <td>31.772499</td>\n",
       "      <td>31.264999</td>\n",
       "      <td>31.475000</td>\n",
       "      <td>28.576729</td>\n",
       "      <td>9.517400e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.937500</td>\n",
       "      <td>41.432499</td>\n",
       "      <td>40.602501</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.263371</td>\n",
       "      <td>1.211508e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.125000</td>\n",
       "      <td>47.424999</td>\n",
       "      <td>46.695000</td>\n",
       "      <td>47.037498</td>\n",
       "      <td>45.263882</td>\n",
       "      <td>1.691264e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.062500</td>\n",
       "      <td>80.832497</td>\n",
       "      <td>79.379997</td>\n",
       "      <td>79.807503</td>\n",
       "      <td>78.315315</td>\n",
       "      <td>1.460852e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open          High           Low         Close     Adj Close  \\\n",
       "count  15965.000000  15965.000000  15965.000000  15965.000000  15965.000000   \n",
       "mean      40.583061     40.952148     40.241173     40.605005     38.739098   \n",
       "std       11.884583     11.980327     11.799389     11.891820     12.158320   \n",
       "min       13.856071     14.271429     13.753571     13.947500     12.084597   \n",
       "25%       31.522499     31.772499     31.264999     31.475000     28.576729   \n",
       "50%       40.937500     41.432499     40.602501     41.000000     39.263371   \n",
       "75%       47.125000     47.424999     46.695000     47.037498     45.263882   \n",
       "max       80.062500     80.832497     79.379997     79.807503     78.315315   \n",
       "\n",
       "             Volume  \n",
       "count  1.596500e+04  \n",
       "mean   1.536463e+08  \n",
       "std    1.096033e+08  \n",
       "min    4.544800e+07  \n",
       "25%    9.517400e+07  \n",
       "50%    1.211508e+08  \n",
       "75%    1.691264e+08  \n",
       "max    1.460852e+09  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Remove unnecessary column\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Step 2: Remove duplicate texts\n",
    "data.drop_duplicates(subset=['title', 'content'], inplace=True)\n",
    "\n",
    "# Step 3: Remove rows with large amount of spaces or empty texts in 'title' and 'content'\n",
    "data = data[~data['title'].str.isspace()]\n",
    "data = data[~data['content'].str.isspace()]\n",
    "data.dropna(subset=['title', 'content'], inplace=True)\n",
    "\n",
    "# Check the dataframe after these preprocessing steps\n",
    "data.info()\n",
    "\n",
    "# Step 5: Check for invalid numeric data\n",
    "numeric_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "data[numeric_columns].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) set proxy\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source ~/clash_dir/set && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "output\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 22:31:42.750261: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-01 22:31:42.795731: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 22:31:42.795753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 22:31:42.797140: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 22:31:42.806481: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 22:31:43.837330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the FinBERT model and tokenizer\n",
    "checkpoint = 'yiyanghkust/finbert-tone'\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "# Create a pipeline for sentiment analysis\n",
    "# Do not truncate the original text\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑动窗口方法\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 根据token数量来切分\n",
    "def sliding_window(text, max_len, overlap, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    text (str): the text to be split into chunks\n",
    "    max_len (int): the maximum length of each chunk\n",
    "    overlap (int): the number of overlapped tokens between chunks\n",
    "    tokenizer: the tokenizer used to tokenize the text\n",
    "\n",
    "    Returns:\n",
    "    list of str: the list of text chunks\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_len - overlap):\n",
    "        chunk = tokens[i:i + max_len]\n",
    "        chunk = tokenizer.convert_tokens_to_string(chunk)\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def process_batch(texts, nlp, max_len, overlap, tokenizer):\n",
    "    sentiments = []\n",
    "    for text in texts:\n",
    "        # Apply sliding window to the text\n",
    "        text_chunks = sliding_window(text, max_len, overlap, tokenizer)\n",
    "\n",
    "        # Apply sentiment analysis to each chunk\n",
    "        chunk_sentiments = []\n",
    "        chunk_weights = []\n",
    "        for chunk in text_chunks:\n",
    "            result = nlp(chunk)\n",
    "            sentiment = result[0]['label']\n",
    "            chunk_sentiments.append(sentiment)\n",
    "            # Use the length of the chunk as the weight\n",
    "            weight = len(chunk)\n",
    "            chunk_weights.append(weight)\n",
    "\n",
    "        # Combine the sentiments using weighted voting\n",
    "        sentiment_counter = Counter()\n",
    "        for sentiment, weight in zip(chunk_sentiments, chunk_weights):\n",
    "            sentiment_counter[sentiment] += weight\n",
    "        final_sentiment = sentiment_counter.most_common(1)[0][0]\n",
    "        sentiments.append(final_sentiment)\n",
    "    return sentiments\n",
    "\n",
    "def apply_sentiment_analysis_parallel(df, nlp, tokenizer, text_column='content', max_len=500, overlap=50, num_workers=16, batch_size=10):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to a column in a dataframe using sliding window method.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Dataframe containing the text data.\n",
    "    nlp (pipeline): HuggingFace pipeline for sentiment analysis.\n",
    "    tokenizer: the tokenizer used to tokenize the text\n",
    "    text_column (str): Name of the column containing text data.\n",
    "    max_len (int): The maximum length of each text chunk.\n",
    "    overlap (int): The number of overlapped tokens between chunks.\n",
    "    num_workers (int): The number of threads to use for parallel processing.\n",
    "    batch_size (int): The number of texts to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with a new column 'sentiment' containing the analysis results.\n",
    "    \"\"\"\n",
    "    # Break the texts into batches\n",
    "    text_batches = [df[text_column][i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    sentiments = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_batch, batch, nlp, max_len, overlap, tokenizer) for batch in text_batches]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            try:\n",
    "                sentiments.extend(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Error in processing text: {e}\")\n",
    "                sentiments.extend(['Error'] * batch_size)\n",
    "\n",
    "    # Add the sentiments as a new column in the dataframe\n",
    "    df['sentiment'] = sentiments\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Apply sentiment analysis to the dataset\n",
    "data = apply_sentiment_analysis_parallel(data, nlp, tokenizer)\n",
    "\n",
    "# Step 2: Prepare data for the prediction model\n",
    "# Here we'll assume the sentiment analysis has been applied and 'sentiment' column is added to the data\n",
    "\n",
    "# We might want to convert sentiments to numerical values for model training\n",
    "sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "data['sentiment_numeric'] = data['sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to save the processed DataFrame to a CSV file\n",
    "data.to_csv('./data/dataset_with_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust display settings for better visualization of samples\n",
    "pd.set_option('display.max_colwidth', 200)  # Adjust the width to fit longer texts\n",
    "\n",
    "# Display some random samples with formatted output\n",
    "sample_data = data.sample(n=10)[['content', 'sentiment']]\n",
    "\n",
    "# Print each sample in a more readable format\n",
    "for index, row in sample_data.iterrows():\n",
    "    print(f\"Sample {index}:\")\n",
    "    print(f\"Content: {row['content']}\")\n",
    "    print(f\"Sentiment: {row['sentiment']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame with 'sentiment' and 'label' columns\n",
    "# Calculate the proportion of each sentiment category\n",
    "sentiment_counts = data['sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Calculate the proportion of each label\n",
    "label_counts = data['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Sentiment Distribution (%):\")\n",
    "print(sentiment_counts)\n",
    "print(\"\\nLabel Distribution (%):\")\n",
    "print(label_counts)\n",
    "\n",
    "# For additional insights, we can also look at the cross-tabulation of sentiment and label\n",
    "crosstab = pd.crosstab(data['sentiment'], data['label'], normalize='index') * 100\n",
    "print(\"\\nCross-Tabulation of Sentiment and Label (%):\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read for existed csv\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./data/dataset_with_sentiment.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format and sort the dataframe by 'Date'\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data_sorted = data.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按 'Date' 和 'sentiment' 分组，然后计算每个类别的 category 为news和opinion的数量\n",
    "category_news_per_day_sentiment = data_sorted[data_sorted['category'] == 'news'].groupby(['Date', 'sentiment']).size().unstack().fillna(0)\n",
    "category_opinion_per_day_sentiment = data_sorted[data_sorted['category'] == 'opinion'].groupby(['Date', 'sentiment']).size().unstack().fillna(0)\n",
    "# 分别计算news和opinion的total\n",
    "category_news_total_per_day_sentiment = data_sorted[data_sorted['category'] == 'news'].groupby(['Date']).size()\n",
    "category_opinion_total_per_day_sentiment = data_sorted[data_sorted['category'] == 'opinion'].groupby(['Date']).size()\n",
    "\n",
    "data_sorted = data_sorted.set_index('Date')\n",
    "data_sorted['P_news_pos'] = category_news_per_day_sentiment['Positive'].reindex(data_sorted.index) / category_news_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_news_neg'] = category_news_per_day_sentiment['Negative'].reindex(data_sorted.index) / category_news_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_op_pos'] = category_opinion_per_day_sentiment['Positive'].reindex(data_sorted.index) / category_opinion_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_op_neg'] = category_opinion_per_day_sentiment['Negative'].reindex(data_sorted.index) / category_opinion_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted = data_sorted.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data = data_sorted.groupby('Date').last()\n",
    "\n",
    "# Shift the 'Open' column to get the next day's opening price\n",
    "daily_data['Next_Open'] = daily_data['Open'].shift(-1)\n",
    "\n",
    "# Drop the last row as it will not have a 'Next_Open' value\n",
    "daily_data = daily_data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2016-10-28')\n",
    "daily_data.loc[(date_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2016-10-28')\n",
    "data_sorted.loc[data_sorted['Date'] == date_to_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2020-01-23')\n",
    "daily_data.loc[(date_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2018-05-06')\n",
    "category_to_query = 'news'\n",
    "data_sorted.loc[(data_sorted['Date'] == date_to_query) & (data_sorted['category'] == category_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data['P_news_neg'].fillna(0, inplace=True)\n",
    "daily_data['P_news_pos'].fillna(0, inplace=True)\n",
    "daily_data['P_op_neg'].fillna(0, inplace=True)\n",
    "daily_data['P_op_pos'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the dataset into a Pandas DataFrame\n",
    "historical_data = pd.read_csv('data/AAPL_Yahoo_Correct.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "\n",
    "# Plotting the 'Open' price against the 'Date'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(historical_data['Date'], historical_data['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Date将historical_data的全部列和daily_data的这四个P_开头的列合并。如果出现有些天在daily_data中不存在，则四个P_开头的列在这一天都置为0。\n",
    "daily_data_merged = pd.merge(historical_data, daily_data[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']], left_on='Date', right_index=True, how='left')\n",
    "daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']] = daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(daily_data_merged['Date'], daily_data_merged['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.to_csv('./data/dataset_FinBERT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.1: VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the dataset into a Pandas DataFrame\n",
    "historical_data = pd.read_csv('data/AAPL_Yahoo_Correct.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "\n",
    "# Plotting the 'Open' price against the 'Date'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(historical_data['Date'], historical_data['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read VADER results from csv\n",
    "# vader_daily_results = pd.read_csv('./data/VADER_results.csv')\n",
    "# vader_daily_results = pd.read_csv('./data/combined_data_mean5.csv')\n",
    "# vader_daily_results = pd.read_csv('./data/combined_data_mean_first512.csv')\n",
    "vader_daily_results = pd.read_csv('./data/proportion_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_daily_results['Date'] = pd.to_datetime(vader_daily_results['Date'])\n",
    "vader_daily_results = vader_daily_results[['Date', 'news_neg', 'news_pos', 'opinion_neg', 'opinion_pos']]\n",
    "vader_daily_results.columns = ['Date', 'P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']\n",
    "vader_daily_results.set_index('Date', inplace=True)\n",
    "\n",
    "daily_data_merged = pd.merge(historical_data, vader_daily_results[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']], left_on='Date', right_index=True, how='left')\n",
    "daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']] = daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.to_csv('./data/dataset_VADER.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Stock price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# FinBERT\n",
    "daily_data_merged = pd.read_csv('./data/dataset_FinBERT.csv', index_col=0)\n",
    "# VADER\n",
    "# daily_data_merged = pd.read_csv('./data/dataset_VADER.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Showing sentiment analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment_results = daily_data_merged[['Date', 'P_news_pos', 'P_news_neg', 'P_op_pos', 'P_op_neg']]\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the data into two parts: news sentiment analysis results and opinion sentiment analysis results\n",
    "news_sentiment_results = daily_sentiment_results[['Date', 'P_news_pos', 'P_news_neg']]\n",
    "opinion_sentiment_results = daily_sentiment_results[['Date', 'P_op_pos', 'P_op_neg']]\n",
    "\n",
    "# Ensure 'Date' is in news_sentiment_results and opinion_sentiment_results\n",
    "assert 'Date' in news_sentiment_results.columns\n",
    "assert 'Date' in opinion_sentiment_results.columns\n",
    "\n",
    "# Set 'Date' column as index\n",
    "news_sentiment_results.set_index('Date', inplace=True)\n",
    "opinion_sentiment_results.set_index('Date', inplace=True)\n",
    "\n",
    "# Draw a heatmap for news sentiment analysis results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('News Sentiment Analysis Results')\n",
    "sns.heatmap(news_sentiment_results.tail(5), annot=True, cmap='YlGnBu', fmt=\".3f\")\n",
    "plt.show()\n",
    "\n",
    "# Draw a heatmap for opinion sentiment analysis results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Opinion Sentiment Analysis Results')\n",
    "sns.heatmap(opinion_sentiment_results.tail(5), annot=True, cmap='YlGnBu', fmt=\".3f\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择特征和目标\n",
    "# 保留原本的index，将'Date'列单独提取出来保存\n",
    "date = daily_data_merged['Date']\n",
    "date = pd.to_datetime(date)\n",
    "\n",
    "features = daily_data_merged.drop(['Date'], axis=1)\n",
    "# Open作为预测目标\n",
    "target = daily_data_merged['Open']\n",
    "features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply the MinMaxScaler to the features and target\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# fit_transform根据数据计算缩放参数\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# 保存缩放参数\n",
    "import joblib\n",
    "joblib.dump(scaler_features, './model/scaler_features.pkl')\n",
    "joblib.dump(scaler_target, './model/scaler_target.pkl')\n",
    "\n",
    "# Create new DataFrames with the scaled features and target\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "scaled_target_df = pd.DataFrame(scaled_target, columns=['Open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features, targets, seq_length):\n",
    "    \"\"\"\n",
    "    Create sequences of specified length from time series data.\n",
    "\n",
    "    Args:\n",
    "    features (np.array): The feature data.\n",
    "    targets (np.array): The target data.\n",
    "    seq_length (int): The length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Sequences of features.\n",
    "    np.array: Corresponding targets for each sequence.\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        x = features[i:(i + seq_length)]\n",
    "        y = targets[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length\n",
    "seq_length = 25\n",
    "\n",
    "# Create sequences\n",
    "features_seq, target_seq = create_sequences(scaled_features, scaled_target, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "    features_seq, target_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_features, test_features, val_target, test_target = train_test_split(\n",
    "    test_features, test_target, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences to Tensor\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_target = torch.tensor(train_target, dtype=torch.float32)\n",
    "\n",
    "val_features = torch.tensor(val_features, dtype=torch.float32)\n",
    "val_target = torch.tensor(val_target, dtype=torch.float32)\n",
    "\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_target = torch.tensor(test_target, dtype=torch.float32)\n",
    "\n",
    "# 创建TensorDataset\n",
    "train_dataset = TensorDataset(train_features, train_target)\n",
    "val_dataset = TensorDataset(val_features, val_target)\n",
    "test_dataset = TensorDataset(test_features, test_target)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用普通的LSTM模型，不使用注意力机制\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_layers, output_dim, dropout=0.2):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 打印lstm_out的形状\n",
    "        # print(lstm_out.shape)\n",
    "        # 取最后一个时间步的输出\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用注意力机制的LSTM\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.attn = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         attn_weights = torch.tanh(self.attn(encoder_outputs))\n",
    "#         return torch.bmm(attn_weights.transpose(1, 2), encoder_outputs).squeeze(1)\n",
    "\n",
    "# class AttentionLSTM(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_size, num_layers, output_dim, dropout=0.2):\n",
    "#         super(AttentionLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         # LSTM层\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_size, num_layers, \n",
    "#                             batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "#         # 注意力层\n",
    "#         self.attention = Attention(hidden_size)\n",
    "        \n",
    "#         # 全连接层\n",
    "#         self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         attn_out = self.attention(lstm_out[:, -1, :], lstm_out)\n",
    "#         output = self.fc(attn_out)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "input_dim = scaled_features_df.shape[1]  # 特征数量\n",
    "hidden_size = 100  # 隐藏状态中的特征数量，可以调整\n",
    "num_layers = 4    # 堆叠的LSTM层的数量\n",
    "output_dim = 1    # 输出维度的数量（预测一个值）\n",
    "\n",
    "# 使用SimpleLSTM\n",
    "model = SimpleLSTM(input_dim, hidden_size, num_layers, output_dim, dropout=0.2)\n",
    "# 使用AttentionLSTM\n",
    "# model = AttentionLSTM(input_dim, hidden_size, num_layers, output_dim, dropout=0.2)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './model/best_model.pth')\n",
    "        print('best_model updated at epoch {}, best_val_loss : {:.4f}'.format(epoch+1, best_val_loss))\n",
    "        \n",
    "    # 每5轮打印一次train loss和val loss\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # 记录两个loss\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "# 在所有epochs结束后绘制损失图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_epochs), train_loss_list, label='Train Loss', color='blue')\n",
    "plt.plot(range(num_epochs), val_loss_list, label='Validation Loss', color='red')\n",
    "plt.title('Train Loss and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load('./model/best_model.pth'))\n",
    "\n",
    "# 计算新的测试集的大小\n",
    "test_size_new = int(len(features_seq) * 0.05)\n",
    "\n",
    "# 按时间顺序划分新的测试集\n",
    "test_features_new, test_target_new = features_seq[-test_size_new:], target_seq[-test_size_new:]\n",
    "\n",
    "# 使用模型进行预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions_new = model(torch.tensor(test_features_new, dtype=torch.float32)).numpy()\n",
    "\n",
    "# 反缩放预测值\n",
    "test_predictions_new = scaler_target.inverse_transform(test_predictions_new).flatten()\n",
    "\n",
    "# 反缩放真实目标值\n",
    "test_target_new = scaler_target.inverse_transform(test_target_new.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 计算日期的总长度\n",
    "total_length = len(date)\n",
    "\n",
    "# 计算测试集的开始位置\n",
    "test_start = total_length - test_size_new\n",
    "\n",
    "# 计算新的测试集的结束位置\n",
    "test_end = total_length\n",
    "\n",
    "# 获取新的测试集的日期范围\n",
    "test_date_new = date[test_start:test_end]\n",
    "\n",
    "# Print the date range of the new test set\n",
    "print(\"The date range of the new test set is from\", test_date_new.iloc[0], \"to\", test_date_new.iloc[-1])\n",
    "\n",
    "# Print the length of the new test set\n",
    "print(\"The length is\", len(test_target_new))\n",
    "\n",
    "# 绘制实际股价和预测股价的对比图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(date[test_start:], test_target_new, label='Actual Prices', color='blue')\n",
    "plt.plot(date[test_start:], test_predictions_new, label='Predicted Prices', color='red')\n",
    "\n",
    "# 设置x轴的日期格式\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=10))  # 设置日期间隔\n",
    "\n",
    "plt.title('Predicted vs Actual Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.gcf().autofmt_xdate()  # 自动调整x轴日期标签的角度以提高可读性\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 计算MSE\n",
    "mse = mean_squared_error(test_target_new, test_predictions_new)\n",
    "print('Test MSE: ', mse)\n",
    "# 计算RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print('Test RMSE: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 50个epoch RMSE测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建一个数据框来显示序列长度和对应的RMSE\n",
    "seq_rmse_df = pd.DataFrame({\n",
    "    'Seq_length': [10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'RMSE': [1.1130488933158105, 0.8456300018583505, 0.8641183126757913, 0.8094936259619585, 0.9580832258849209, 0.8409732222581049, 0.9262937021912081, 1.1064503987204368, 0.9005207820679706]\n",
    "})\n",
    "\n",
    "# 将RMSE保留两位小数\n",
    "seq_rmse_df['RMSE'] = seq_rmse_df['RMSE'].round(3)\n",
    "seq_rmse_df = seq_rmse_df.reset_index(drop=True)\n",
    "\n",
    "import matplotlib.cm\n",
    "from plottable import ColumnDefinition, Table\n",
    "\n",
    "# 创建列定义\n",
    "seq_length_col_def = ColumnDefinition('Seq_length', title='sequence length')\n",
    "rmse_col_def = ColumnDefinition('RMSE', title='RMSE', cmap=matplotlib.cm.get_cmap('viridis'))\n",
    "\n",
    "# 创建表格\n",
    "table = Table(seq_rmse_df, column_definitions=[seq_length_col_def, rmse_col_def])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "rmse_results = [1.1130488933158105, 0.8456300018583505, 0.8641183126757913, 0.8094936259619585, 0.9580832258849209, 0.8409732222581049, 0.9262937021912081, 1.1064503987204368, 0.9005207820679706]\n",
    "\n",
    "# 绘制RMSE结果图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, rmse_results, label='RMSE Results', color='blue')\n",
    "\n",
    "# 标出最小RMSE点\n",
    "min_rmse_index = np.argmin(rmse_results)\n",
    "plt.plot(seq_lengths[min_rmse_index], rmse_results[min_rmse_index], 'ro')\n",
    "plt.text(seq_lengths[min_rmse_index], rmse_results[min_rmse_index], f'Min RMSE: {rmse_results[min_rmse_index]:.3f}', fontsize=12, ha='right')\n",
    "\n",
    "plt.title('RMSE Results for Different Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Method': ['Historical transaction information', 'FinBERT mixed', 'VADER mixed'],\n",
    "    'RMSE': [0.9793, 0.7341, 0.8601]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 设置表格样式\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 创建一个新的figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 使用seaborn的barplot函数绘制条形图\n",
    "sns.barplot(x='Method', y='RMSE', data=df, palette='viridis')\n",
    "\n",
    "# 设置标题和坐标轴标签\n",
    "plt.title('RMSE Results for Different Methods')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
