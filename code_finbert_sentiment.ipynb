{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Apple Set To Beat Q1 Earnings Estimates  Tech ...</td>\n",
       "      <td>Technology giant Apple   NASDAQ AAPL   is set ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Tech Daily  Intel Results  Netflix Surge  Appl...</td>\n",
       "      <td>The top stories in this digest are Intel s   N...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>7 Monster Stock Market Predictions For The Wee...</td>\n",
       "      <td>S P 500  SPY \\nThis week will be packed with e...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Apple Earnings Preview  5G Launch  Expanding S...</td>\n",
       "      <td>Reports Q1 2020 results on Tuesday  Jan  28 ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Buy Surging Apple   Microsoft Stock Before Qua...</td>\n",
       "      <td>On today s episode of Full Court Finance here ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 ticker        Date category  \\\n",
       "0           0   AAPL  2020-01-27  opinion   \n",
       "1           1   AAPL  2020-01-27  opinion   \n",
       "2           2   AAPL  2020-01-27  opinion   \n",
       "3           3   AAPL  2020-01-27  opinion   \n",
       "4           4   AAPL  2020-01-27  opinion   \n",
       "\n",
       "                                               title  \\\n",
       "0  Apple Set To Beat Q1 Earnings Estimates  Tech ...   \n",
       "1  Tech Daily  Intel Results  Netflix Surge  Appl...   \n",
       "2  7 Monster Stock Market Predictions For The Wee...   \n",
       "3  Apple Earnings Preview  5G Launch  Expanding S...   \n",
       "4  Buy Surging Apple   Microsoft Stock Before Qua...   \n",
       "\n",
       "                                             content       Open       High  \\\n",
       "0  Technology giant Apple   NASDAQ AAPL   is set ...  77.514999  77.942497   \n",
       "1  The top stories in this digest are Intel s   N...  77.514999  77.942497   \n",
       "2  S P 500  SPY \\nThis week will be packed with e...  77.514999  77.942497   \n",
       "3    Reports Q1 2020 results on Tuesday  Jan  28 ...  77.514999  77.942497   \n",
       "4  On today s episode of Full Court Finance here ...  77.514999  77.942497   \n",
       "\n",
       "         Low      Close  Adj Close     Volume  label  \n",
       "0  76.220001  77.237503  75.793358  161940000      0  \n",
       "1  76.220001  77.237503  75.793358  161940000      0  \n",
       "2  76.220001  77.237503  75.793358  161940000      0  \n",
       "3  76.220001  77.237503  75.793358  161940000      0  \n",
       "4  76.220001  77.237503  75.793358  161940000      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './data/NEWS_YAHOO_stock_prediction.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Technology giant Apple   NASDAQ AAPL   is set ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>The top stories in this digest are Intel s   N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>S P 500  SPY \\nThis week will be packed with e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Reports Q1 2020 results on Tuesday  Jan  28 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>On today s episode of Full Court Finance here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Monday  January 27  2020The Zacks Research Dai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>news</td>\n",
       "      <td>By Peter Nurse \\nInvesting com   European stoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>news</td>\n",
       "      <td>BTIG analyst Mark Palmer initiates coverage of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>opinion</td>\n",
       "      <td>I got a great question recently from Barbara P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>opinion</td>\n",
       "      <td>We get into the heart of the Q4 earnings seaso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date category                                            content\n",
       "0  2020-01-27  opinion  Technology giant Apple   NASDAQ AAPL   is set ...\n",
       "1  2020-01-27  opinion  The top stories in this digest are Intel s   N...\n",
       "2  2020-01-27  opinion  S P 500  SPY \\nThis week will be packed with e...\n",
       "3  2020-01-27  opinion    Reports Q1 2020 results on Tuesday  Jan  28 ...\n",
       "4  2020-01-27  opinion  On today s episode of Full Court Finance here ...\n",
       "5  2020-01-27  opinion  Monday  January 27  2020The Zacks Research Dai...\n",
       "6  2020-01-27     news  By Peter Nurse \\nInvesting com   European stoc...\n",
       "7  2020-01-27     news  BTIG analyst Mark Palmer initiates coverage of...\n",
       "8  2020-01-24  opinion  I got a great question recently from Barbara P...\n",
       "9  2020-01-24  opinion  We get into the heart of the Q4 earnings seaso..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Date', 'category', 'content']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15965 entries, 0 to 15974\n",
      "Data columns (total 12 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ticker     15965 non-null  object \n",
      " 1   Date       15965 non-null  object \n",
      " 2   category   15965 non-null  object \n",
      " 3   title      15965 non-null  object \n",
      " 4   content    15965 non-null  object \n",
      " 5   Open       15965 non-null  float64\n",
      " 6   High       15965 non-null  float64\n",
      " 7   Low        15965 non-null  float64\n",
      " 8   Close      15965 non-null  float64\n",
      " 9   Adj Close  15965 non-null  float64\n",
      " 10  Volume     15965 non-null  int64  \n",
      " 11  label      15965 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>1.596500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.583061</td>\n",
       "      <td>40.952148</td>\n",
       "      <td>40.241173</td>\n",
       "      <td>40.605005</td>\n",
       "      <td>38.739098</td>\n",
       "      <td>1.536463e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.884583</td>\n",
       "      <td>11.980327</td>\n",
       "      <td>11.799389</td>\n",
       "      <td>11.891820</td>\n",
       "      <td>12.158320</td>\n",
       "      <td>1.096033e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.856071</td>\n",
       "      <td>14.271429</td>\n",
       "      <td>13.753571</td>\n",
       "      <td>13.947500</td>\n",
       "      <td>12.084597</td>\n",
       "      <td>4.544800e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.522499</td>\n",
       "      <td>31.772499</td>\n",
       "      <td>31.264999</td>\n",
       "      <td>31.475000</td>\n",
       "      <td>28.576729</td>\n",
       "      <td>9.517400e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.937500</td>\n",
       "      <td>41.432499</td>\n",
       "      <td>40.602501</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.263371</td>\n",
       "      <td>1.211508e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.125000</td>\n",
       "      <td>47.424999</td>\n",
       "      <td>46.695000</td>\n",
       "      <td>47.037498</td>\n",
       "      <td>45.263882</td>\n",
       "      <td>1.691264e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.062500</td>\n",
       "      <td>80.832497</td>\n",
       "      <td>79.379997</td>\n",
       "      <td>79.807503</td>\n",
       "      <td>78.315315</td>\n",
       "      <td>1.460852e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open          High           Low         Close     Adj Close  \\\n",
       "count  15965.000000  15965.000000  15965.000000  15965.000000  15965.000000   \n",
       "mean      40.583061     40.952148     40.241173     40.605005     38.739098   \n",
       "std       11.884583     11.980327     11.799389     11.891820     12.158320   \n",
       "min       13.856071     14.271429     13.753571     13.947500     12.084597   \n",
       "25%       31.522499     31.772499     31.264999     31.475000     28.576729   \n",
       "50%       40.937500     41.432499     40.602501     41.000000     39.263371   \n",
       "75%       47.125000     47.424999     46.695000     47.037498     45.263882   \n",
       "max       80.062500     80.832497     79.379997     79.807503     78.315315   \n",
       "\n",
       "             Volume  \n",
       "count  1.596500e+04  \n",
       "mean   1.536463e+08  \n",
       "std    1.096033e+08  \n",
       "min    4.544800e+07  \n",
       "25%    9.517400e+07  \n",
       "50%    1.211508e+08  \n",
       "75%    1.691264e+08  \n",
       "max    1.460852e+09  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Remove unnecessary column\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Step 2: Remove duplicate texts\n",
    "data.drop_duplicates(subset=['title', 'content'], inplace=True)\n",
    "\n",
    "# Step 3: Remove rows with large amount of spaces or empty texts in 'title' and 'content'\n",
    "data = data[~data['title'].str.isspace()]\n",
    "data = data[~data['content'].str.isspace()]\n",
    "data.dropna(subset=['title', 'content'], inplace=True)\n",
    "\n",
    "# Check the dataframe after these preprocessing steps\n",
    "data.info()\n",
    "\n",
    "# Step 5: Check for invalid numeric data\n",
    "numeric_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "data[numeric_columns].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) set proxy\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source ~/clash_dir/set && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "output\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 01:31:57.693335: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-02 01:31:57.746328: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 01:31:58.421402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the FinBERT model and tokenizer\n",
    "checkpoint = 'yiyanghkust/finbert-tone'\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "# Create a pipeline for sentiment analysis\n",
    "# Do not truncate the original text\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑动窗口方法\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# 根据token数量来切分\n",
    "def sliding_window(text, max_len, overlap, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    text (str): the text to be split into chunks\n",
    "    max_len (int): the maximum length of each chunk\n",
    "    overlap (int): the number of overlapped tokens between chunks\n",
    "    tokenizer: the tokenizer used to tokenize the text\n",
    "\n",
    "    Returns:\n",
    "    list of str: the list of text chunks\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_len - overlap):\n",
    "        chunk = tokens[i:i + max_len]\n",
    "        chunk = tokenizer.convert_tokens_to_string(chunk)\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def process_batch(texts, nlp, max_len, overlap, tokenizer):\n",
    "    sentiments = []\n",
    "    for text in texts:\n",
    "        # Apply sliding window to the text\n",
    "        text_chunks = sliding_window(text, max_len, overlap, tokenizer)\n",
    "\n",
    "        # Apply sentiment analysis to each chunk\n",
    "        chunk_sentiments = []\n",
    "        chunk_weights = []\n",
    "        for chunk in text_chunks:\n",
    "            result = nlp(chunk)\n",
    "            sentiment = result[0]['label']\n",
    "            chunk_sentiments.append(sentiment)\n",
    "            # Use the length of the chunk as the weight\n",
    "            weight = len(chunk)\n",
    "            chunk_weights.append(weight)\n",
    "\n",
    "        # Combine the sentiments using weighted voting\n",
    "        sentiment_counter = Counter()\n",
    "        for sentiment, weight in zip(chunk_sentiments, chunk_weights):\n",
    "            sentiment_counter[sentiment] += weight\n",
    "        final_sentiment = sentiment_counter.most_common(1)[0][0]\n",
    "        sentiments.append(final_sentiment)\n",
    "    return sentiments\n",
    "\n",
    "\n",
    "def apply_sentiment_analysis_parallel(df, nlp, tokenizer, text_column='content', max_len=500, overlap=50, num_workers=16, batch_size=10):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to a column in a dataframe using sliding window method.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Dataframe containing the text data.\n",
    "    nlp (pipeline): HuggingFace pipeline for sentiment analysis.\n",
    "    tokenizer: the tokenizer used to tokenize the text\n",
    "    text_column (str): Name of the column containing text data.\n",
    "    max_len (int): The maximum length of each text chunk.\n",
    "    overlap (int): The number of overlapped tokens between chunks.\n",
    "    num_workers (int): The number of threads to use for parallel processing.\n",
    "    batch_size (int): The number of texts to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with a new column 'sentiment' containing the analysis results.\n",
    "    \"\"\"\n",
    "    # Break the texts into batches\n",
    "    text_batches = [df[text_column][i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    sentiments = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        for batch_result in tqdm(executor.map(lambda batch: process_batch(batch, nlp, max_len, overlap, tokenizer), text_batches), total=len(text_batches)):\n",
    "            sentiments.extend(batch_result)\n",
    "\n",
    "    # Add the sentiments as a new column in the dataframe\n",
    "    df['sentiment'] = sentiments\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hesirui/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1d08500b384c64b1f1b5e66a3ebd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Apply sentiment analysis to the dataset\n",
    "data = apply_sentiment_analysis_parallel(data, nlp, tokenizer)\n",
    "\n",
    "# Step 2: Prepare data for the prediction model\n",
    "# Here we'll assume the sentiment analysis has been applied and 'sentiment' column is added to the data\n",
    "\n",
    "# We might want to convert sentiments to numerical values for model training\n",
    "sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "data['sentiment_numeric'] = data['sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to save the processed DataFrame to a CSV file\n",
    "data.to_csv('./data/dataset_sliding_window.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 9986:\n",
      "Content: Investing com   U S  stocks edged higher Thursday as Fed Chair Janet Yellen fielded questions from Senators The DJI was up 0 01  at 10 30 ET  The S P 500 added 0 05  The tech heavy Nasdaq composite was up 0 07  The dollar index steadied after earlier weakness that saw it drop to 10 month lows Initial weekly jobless claims fell for the first time in a month  PPI data mixed Oil seesawed as investors weighed the prospects for re balancing supply and demand  NASDAQ Apple  was up 0 65  \n",
      "Sentiment: Positive\n",
      "\n",
      "Sample 11157:\n",
      "Content: Agenus Inc    NASDAQ AGEN   reported first quarter 2017 loss of 18 cents per share  including non cash expenses   narrower than the Zacks Consensus Estimate of a loss of 32 cents and the year ago loss of 37 cents  The decline was due to the accelerated milestone payment received from Incyte Corporation  \n",
      "However  revenues were  26 96 million and surpassed the Zacks Consensus Estimate of  5 million  Revenues were considerably up from  5 96 million in the year ago quarter \n",
      "Agenus  share price decreased 1 0  year to date as against the Zacks classified  industry s gain of 4 7  \n",
      "\n",
      "Quarterly Highlights\n",
      "Agenus  fourth quarter research and development  R D  expenses were up 30 4  to  32 6 million  But general and administrative expenses were down 15 8  to  7 8 million \n",
      "Pipeline Update\n",
      "Agenus is progressing well with the candidates in its pipeline  Currently  the company is evaluating AGEN1884 in a phase I study  It expects to report early safety and efficacy data on AGEN1884 at the upcoming ASCO conference in 2017 \n",
      "Additionally  the company is evaluating INCAGN01876 in a phase I study for the treatment of solid tumors \n",
      "Notably  GlaxoSmithKline   NYSE GSK   filed for regulatory approval of Shingrix vaccine which contains Agenus  QS 21 Stimulon  In fact  GSK s shingles has been filed for regulatory approval in the US  Canada  the EU and also in Japan  The final position of the agency is expected in the second part of this year \n",
      "The company plans to initiate studies on anti PD 1 antagonist AGEN2034 in the first half of 2017   Agenus also plans to begin combination studies on AGEN2034 and AGEN1884 in the second half of the year  Already  it has initiated phase I trial for AutoSynVax in Apr 2017 and completed enrolment as well Agenus Inc  Price  Consensus and EPS Surprise\n",
      "    Zacks Rank   Stocks to Consider\n",
      "Agenus currently carries a Zacks Rank  3  Hold   A top ranked stock in the health care sector includes Galena Biopharma  Inc    NASDAQ GALE   sporting a Zacks Rank  1  Strong Buy   You can see  \n",
      "Galena s loss per share estimates narrowed from  2 03 to 58 cents for 2017 over the last 60 days  The company posted positive earnings surprises in two of the four trailing quarters  with an average beat of 53 83  \n",
      "More Stock News  8 Companies Verge on Apple Like Run\n",
      "Did you miss Apple  NASDAQ AAPL  s 9X stock explosion after they launched their iPhone in 2007  Now 2017 looks to be a pivotal year to get in on another emerging technology expected to rock the market  Demand could soar from almost nothing to  42 billion by 2025  Reports suggest it could save 10 million lives per decade which could in turn save  200 billion in U S  healthcare costs \n",
      "A bonus Zacks Special Report names this breakthrough and the 8 best stocks to exploit it  Like Apple in 2007  these companies are already strong and coiling for potential mega gains \n",
      "Sentiment: Positive\n",
      "\n",
      "Sample 15244:\n",
      "Content: This Stock Could Make 420  Gains as America s Presidential Battle Rages   2016 is already forecast to be the most expensive election of our lives as America drowns in political advertising  But this profit play won t just resist the election year chaos   it actually feeds off it  I m projecting 420  gains for this small cap stock once the shouting s over   and  shows you exactly why \n",
      "Apple Inc   NASDAQ AAPL  has been positively pounded over the past few months  and the stock is down more than 25  from a peak of  132 97 per share  Icahn s out  insiders are bailing  and iPhone sales are slowing  the list of  failures  is growing longer by the minute  or so goes conventional wisdom \n",
      "I m not particularly worried   Apple is not going to  pull a BlackBerry  NASDAQ BBRY   any time soon \n",
      "Let me show you why \n",
      "What Gives \n",
      "Basically we have a situation where the investing public still identifies with Apple as a growth company driven by devices at a time when CEO Tim Cook thinks of it in terms of an  ecosphere  \n",
      "It s a fundamental mismatch based on perception  not reality  and  as a result  a great buying opportunity \n",
      "The company is priced as if it s going to go out of existence based on a 16  decline in iPhone sales during Q1 2016  And yet \n",
      "Apple has enough cash on hand to sustain current dividends and operations for a  generation   according to NYU professor Aswath Damodaran  who is a valuation expert  which jibes with my numbers \n",
      "The company has more than doubled profits from app sales and services to  6 billion by Q2 2016  according to MarketWatch \n",
      "Apple has made several key investments in online offline businesses in China  the most significant of which was an investment in Didi Chuxing  and has signed key agreements with China UnionPay in a move that will allow it to compete with Alibaba Group Holdings  NYSE BABA   NYSE BABA  \n",
      "What s more  the tech giant has teamed up with International Business Machines  NYSE IBM  to generate iOS corporate sales to business app clients \n",
      "The Apple bears don t seem to have caught on to any of this yet  That s something we can use to our advantage \n",
      "You see  I believe it s possible these shares could hit  200 in the next 24 to 36 months as public perception catches up with the reality Tim Cook   Co  are building today \n",
      "Ninety nine percent of investors don t realize what s happening as the  Apple ecosphere  becomes reality   and that s what makes Apple such a compelling opportunity right now \n",
      "We re not alone in our thinking  incidentally \n",
      "Brian White of Brexil Hamilton reiterated a  Buy  and a  185 price target  UBS s Steven Milunovich was more conservative in a client note  but believes current prices reflect  product cycle bears  and stuck to his initial target of  115  But then he  too  came into my neighborhood noting that  200 a share isn t out of the question if the company hits another hardware home run \n",
      "It s important to remember that Apple has a long history of reinventing itself at times when the market least expects it to  which is why you want to concentrate not on the devices but on the higher margin services it offers \n",
      "Not many people realize this  but Apple now makes more money selling services than it does selling Macs and iPads  What s more  services revenue contributed  6 billion to the top line in the most recent quarter  which represents a 20  increase from the same quarter a year ago \n",
      "That s not even the best part \n",
      "Here s the Real Key\n",
      "Apple s service business profit margins grew by 60  in 2015  and is growing even faster now as management continues to leverage its user base of   say it with me   more than 1 billion Apple devices \n",
      "Services don t cost a lot to acquire  nor do they cost a lot to provide  which means  of course  that more money comes to the bottom line \n",
      "This is what Cook sees and what the vast majority of the investing public does not  yet \n",
      "So  if you don t already own Apple  now s the time to get on board or risk getting left behind  It offers one of the best opportunities on the market to play one of my favorite Unstoppable Trends   Technology \n",
      "How Keith Does It\n",
      "Technology is one of just six lucrative  Unstoppable Trends  Keith tracks to bring his readers some of the best investment recommendations on the markets today  These trends have been making fortunes for centuries  and they ll likely continue to do so throughout the 21st century \n",
      "These Unstoppable Trends can t be derailed by politics or market turmoil  so they ve helped Keith and our entire Money Map team bring peak gains of 818  on Pharmacyclics and 543  on Westport Innovations  for instance  no matter what the markets were doing at the time \n",
      "Sentiment: Positive\n",
      "\n",
      "Sample 7059:\n",
      "Content: Cerner Corporation   NASDAQ CERN   announced that it is collaborating with Virginia based Surescripts  an information technology company that provides network to pharmacies for connecting fragmented health information technology  With this deal  Cerner will be able to integrate Surescripts  Real Time Prescription Benefit functionality into the  Cerner Millennium  electronic health record  EHR  In this regard we note that  last November  the company along with other EHR makers had entered into another collaboration with Surescripts on personalized prescription benefit This development will help in providing patient specific prescription information required at any point of time during the physician s workflow Cerner Corporation Price and Consensus    Cerner Millennium  is an EHR platform used to reduce medical error  It helps physicians document and access critical patient data and organize workflows  thereby ensuring patient safety through evidence based decision making Per management  the functionality will be implemented in March 2018 and will be added to Cerner ePrescribe  which comes free of cost for all clients This alliance will thus help Cerner gain a stronger foothold in the HCIT  healthcare information technology  space Why is Real Time Prescription Benefit Important Surescripts  Real Time Prescription Benefit provides prescription price transparency by leveraging information from pharmacy benefit managers  This process helps identify other therapeutic alternatives and patient specific coverage alerts like quantity restrictions and step therapy requirements Consequently  this will reduce the number of patients abandoning their prescriptions if the prescribed drugs become unaffordable for them  Hence  greater price transparency may improve medication adherence and also build stronger relationships between the patients and physicians Lucrative Prospects in Niche SpacePer a study conducted by Allied Market Research  North America was the leading regional HCIT market in 2015  The North American healthcare asset management market is projected to witness a CAGR of 17 4  to  9 7 billion by 2022 Rising needs to curtail healthcare costs and growing focus on quality of care drive the market Accordingly  we believe  the addition of the new functionality in EHR platform is a strategic move by Cerner as it is likely to secure a diversified client base in the near future Price PerformanceIn the past year  Cerner has underperformed the  in terms of price  The stock has returned 14 4  compared with the industry s rally of 16 6  However  we are optimistic that the latest developments will have a positive impact on Cerner s share price movement Zacks Ranks   Key PicksCerner carries a Zacks Rank  3  Hold  Some better ranked stocks in the broader medical space are Centene Corporation   NYSE CNC    Bioverativ Inc   NASDAQ BIVV   and athenahealth  Inc    NASDAQ ATHN    Each of these stocks sports a Zacks Rank  1  Strong Buy   You can see Centene s projected long term growth rate is 14 4   The stock has returned 16 9  in the past six months Bioverativ has an expected long term growth rate of 14   The stock has returned a whopping 106 8  in the past three months athenahealth has an expected long term growth rate of 20 7   The stock has returned 5 1  in the past three months More Stock News  This Is Bigger than the iPhone  It could become the mother of all technological revolutions  Apple  NASDAQ AAPL  sold a mere 1 billion iPhones in 10 years but a new breakthrough is expected to generate more than 27 billion devices in just 3 years  creating a  1 7 trillion market Zacks has just released a Special Report that spotlights this fast emerging phenomenon and 6 tickers for taking advantage of it  If you don t buy now  you may kick yourself in 2020 \n",
      "Sentiment: Positive\n",
      "\n",
      "Sample 761:\n",
      "Content: Remember  the November episode of the Zacks Ultimate Strategy Session is now available for viewing  Don t miss your chance to hear \n",
      "\n",
      "  Ben Rains and Madeleine Johnson Agree to Disagree on the Streaming Wars and whether Netflix  NASDAQ NFLX  will be affected by new competition from Apple  NASDAQ AAPL  TV   Disney  and more\n",
      "\t  Kevin Matras covers if investors should get in with stocks making new all time highs or wait for a pullback in Zacks Mailbag\n",
      "\t  Sheraz Mian and Kevin Cook choose one portfolio to give feedback for improvement\n",
      "\t  Market conditions from both fundamental and technical views\n",
      "\t  The full list of top performing stocks over the past 30 days\n",
      "\t  New stocks added to the Zacks Ultimate portfolio\n",
      "\t  And much more\n",
      "\n",
      "Simply log on to Zacks com and view the November episode here  And please let us know what you think of these monthly episodes  Email all feedback to mailbag zacks com \n",
      "\tTwo of the major indices saw their three day winning streaks end on Wednesday  but the S P managed a slight gain as the market mostly treaded water in the wake of its record setting run \n",
      "\tThe big news of the day was that the meeting between President Donald Trump and China leader Xi Jinping may not happen until December  It was originally scheduled for later this month at a global summit in Chile  but the country cancelled the whole thing last week \n",
      "\tAs far as the market is concerned  a further out meeting continues the uncertainty and provides more opportunity for this Phase 1 deal to fall through \n",
      "\tHowever  it should be noted that most of the language from the two sides has been positive of late  Let s hope it lasts \n",
      "\tBut even without news of a potentially delayed meeting  this market would probably have been under pressure on Wednesday  Stocks are breaking records all over the place  culminating with all three major indices reaching new highs on Monday \n",
      "\tSo it was time for a break  which began yesterday with stocks selling off in the final hour and continued today with a lackluster session \n",
      "\tThe major indices dipped sharply lower on the trade news  but they fought back before the closing bell  However  the S P was the only one to return to the positive side  finishing with a slight gain of 0 07  to 3076 78 \n",
      "\tThe Dow broke even  though was technically down by 0 07 of a point  or 0   to 27 492 56  The NASDAQ had the worst performance  though it was only off by 0 29   or about 24 points  to 8410 63 \n",
      "\tWe ve seen the kind of damage a bad trade headline can have  and this just isn t it  Most likely  it was an excuse to take a rest after an impressive run higher Today s Portfolio Highlights \n",
      "\tHome Run Investor  The portfolio continues getting more aggressive during this record setting market  which means Brian is adding more technology  On Wednesday  he picked up Tenable Holdings  TENB   a Zacks Rank  2  Buy  provider of Cyber Exposure solutions  In other words  it s a play on  software security  which has seen some good news lately from other players in the space  Plus  TENB has beaten the Zacks Consensus Estimate for four straight quarters with an average surprise of 28  over that time  Earnings estimates are on the rise for this year and next  and the editor expects that TENB s flip to profitability is  only a matter of time   Brian also decided to sell AquaVenture Holdings  WAAS  for a 12 1  return in a month  as the water play beat earnings expectations but just isn t aggressive enough for the portfolio right now  Read more in the full write up Counterstrike  A craft beer is a great way for an investor to celebrate the market s recent record setting run  In a way  that s what this portfolio is doing by adding a 10  allocation in The Boston Beer Company  SAM   which makes the popular Samuel Adams brand  After rising for most of the year  the stock came under serious pressure due to lower margins in the most recent report  It s now dropped to its 200 day  and Jeremy thinks there s  massive support  there  By the way  SAM remains a Zacks Rank  1  Strong Buy   The editor also bought a 4  position in IPath Series B S P 500 VIX Short Term Futures ETN  VXX  as a small hedge for the overall portfolio in the event that a negative headline spikes the VIX  Read the full write up for more on today s moves   TAZR Trader  Shares of GW Pharmaceuticals  GWPH  plunged on Wednesday on concerns of discontinued prescriptions and a slower growth rate of new patients  However  the biopharma company still beat on the top and bottom lines in its report  Regardless of today s dip  Kevin reminds us that GWPH is still a  revolutionary  pharma company that will probably get to sales of  1 billion by 2022  With much of the risk now taken out of the stock  he bought GWPH on Wednesday with a 7  allocation \n",
      "\tMeanwhile  the editor decided that now was good time to play a potential pullback in November with earnings season coming to an end and the VIX looking to rise higher amid stalled momentum  He added 5  allocations in Proshares UltraPro Short QQQ  SQQQ  and ProShares UltraPro Short Dow 30  SDOW  each  These are both ProShares 3X leveraged  bear  ETFs  Kevin also sold a third of Invitae  NVTA  to reduce risk  Read the full write up for more on today s moves  including what analysts think of GWPH and the editor s 4 point rationale for the bearish ETFs Surprise Trader  Usually Dave likes to add stocks a few days or even a week before their quarterly report  but he s got to pounce on opportunities as earnings season begins to die down  Therefore  he added Realogy  RLGY  on the eve of its quarterly report coming before the bell tomorrow  This Zacks Rank  1  Strong Buy  provider of real estate services has a positive Earnings ESP of 2 4  for the quarter and is part of a space  Real Estate   Operations  in the Top 17  of the Zacks Industry Rank  The editor also sold Boot Barn  BOOT  today for a 2 4  return in about 2 weeks  Read the complete commentary for more on today s moves \n",
      "\tAll the Best \n",
      "\tJim Giaquinto \n",
      "Recommendations from Zacks  Private Portfolios   Believe it or not  this article is not available on the Zacks com website  The commentary is a partial overview of the daily activity from Zacks  private recommendation services  If you would like to follow our Buy and Sell signals in real time  we ve made a special arrangement for readers of this website  Starting today you can see all the recommendations from all of Zacks  portfolios absolutely free for 7 days  Our services cover everything from value stocks and momentum trades to insider buying and positive earnings surprises  which we ve predicted with an astonishing 80   accuracy    Click here to  test drive  Zacks Ultimate for FREE   \n",
      "Sentiment: Positive\n",
      "\n",
      "Sample 11103:\n",
      "Content: Twenty First Century Fox  Inc    NASDAQ FOXA   just released its third quarter 2017 financial results  posting earnings of  0 54 per share and revenues of  7 56 billion \n",
      "Currently  FOXA is a Zacks Rank  3  Hold   but this ranking could change based on today s results  The stock is down 2 72  to  27 90 per share in after hours trading shortly after its earnings report was released \n",
      "FOXA  \n",
      "Beat earnings estimates  The company posted earnings of  0 54 per share  excluding  0 11 from non recurring items   beating the Zacks Consensus Estimate of  0 47 per share \n",
      "Missed revenue estimates  The company saw revenue figures of  7 56 billion  missing our consensus estimate of  7 68 billion \n",
      "FOXA s quarterly revenues jumped by 5  year over year  The company attributed much of its revenue growth to higher TV advertising revenue especially from Super Bowl LI \n",
      "Increased affiliate television revenues helped FOXA offset lower content revenues from its 20th Century Fox film segment \n",
      "The company s domestic affiliate revenues rose 8  due in large part to contractual rate increases led by Fox News Channel  FS1  and FX Networks  FOXA s local advertising revenue was flat for the quarter with its National Geographic sector offsetting Fox News and FS1 s strong quarter \n",
      "FOXA s quarterly income from continuing operations fell by  139 million sequentially \n",
      "The media giant remains confident that its proposed buyout of U K  based media company Sky  which was recently approved unconditionally by the European Commission  will be fully approved soon  FOXA expects the acquisition to create a major new revenue stream \n",
      " We delivered a quarter marked by operational momentum and strong domestic affiliate fee growth   FOXA Executive Chairmen Rupert and Lachlan Murdoch said in a joint statement   We continue to demonstrate our ability to capture opportunities to grow distribution of our domestic portfolio of video brands  whether through established MVPD partners or new digital entrants such as Hulu s recently launched live television service \n",
      " We made progress in the quarter against our key strategic priorities  exemplified by our creative successes across screens  from theatrical releases Logan and Hidden Figures to new FX debuts of Legion  Feud and Taboo  \n",
      "Here s a graph that looks at FOXA s Price  Consensus and EPS Surprise history Twenty First Century Fox  Inc  Price  Consensus and EPS Surprise   Twenty First Century Fox  Inc  is involved in creating and distributing media services  Its business portfolio consists of cable  broadcast  film  pay TV and satellite assets  Twenty First Century Fox  Inc   formerly known as News Corporation  is based in New York  United States \n",
      "Check back later for our full analysis on FOXA s earnings report \n",
      "More Stock News  8 Companies Verge on Apple Like Run\n",
      "Did you miss Apple  NASDAQ AAPL  s 9X stock explosion after they launched their iPhone in 2007  Now 2017 looks to be a pivotal year to get in on another emerging technology expected to rock the market  Demand could soar from almost nothing to  42 billion by 2025  Reports suggest it could save 10 million lives per decade  which could in turn save  200 billion in U S  healthcare costs \n",
      "A bonus Zacks Special Report names this breakthrough and the 8 best stocks to exploit it  Like Apple in 2007  these companies are already strong and coiling for potential mega gains \n",
      "Sentiment: Positive\n",
      "\n",
      "Sample 6476:\n",
      "Content: Wireless speaker firm Sonos has filed confidentially for an initial public offering that could value the company between  2 5B and  3B  The Wall Street Journal reports  It may go public as soon as June or July  according to the report   Morgan Stanley   NYSE MS  and Goldman Sachs  NYSE GS  will lead the offering  It s raised about  110M in primary funding  and it expected 2017 revenue to cross  1B  Recently  it s partnered with rivals in the getting smarter speaker space  including Amazon com  NASDAQ AMZN   Apple  NASDAQ AAPL  and Alphabet  GOOG  GOOGL  Sonos released its own smart speaker last year  powered by Amazon s Alexa  though it is working to add compability with Apple s Siri and Alphabet s Google Assistant  Now read \n",
      "Sentiment: Neutral\n",
      "\n",
      "Sample 15793:\n",
      "Content: One more name today that reports tonight  Apple  AAPL   AppleApple has been falling since topping out at 701 in September  The Relative Strength Index  RSI  is trying to move higher but in bearish territory and the Moving Average Convergence Divergence  MACD  indicator is negative but flat  The trend remains lower  There is resistance at 555 and 565 followed by 595  The Hagopian Line has been acting as resistance as well and stands at 530 very near the 50 day Simple moving Average  SMA   Support lower comes at 500 and 483 before 476 and 450  The reaction to the last 6 earnings reports has been a move of about 4 72  on average or  24 25 making for an expected range of 487 to 540  The at the money January Straddles suggest a larger  36 move by Expiry with Implied Volatility at 115  above the next week at 60   Trade Idea 1  Sell the January 445 590 strangle for a  1 credit  Trade Idea 2  Buy the January 540 550 555 broken wing call Butterfly for  1 91  Trade Idea 3  Buy the January 480 470 465 broken wing Put Butterfly for  1 64  Trade Idea 4  Buy both Butterfly s for  3 55  I will not be taking these as some of you may have seen me sell to open a January 470 450 430 put Butterfly  This is now paired against my outstanding February 475 450 425 Put Butterfly and reduces my cost   1 is a good income trade but will eat margin  The good news is that it is only for 2 days if you have that margin   4 allows you to participate either direction on a move of  30  and if it blows out to one side still gives you a 1 45 profit  Disclosure  The information in this blog post represents my own opinions and does not contain a recommendation for any particular security or investment  I or my affiliates may hold positions or other interests in securities mentioned in the Blog  please see my  page for my full disclaimer \n",
      "Sentiment: Neutral\n",
      "\n",
      "Sample 10724:\n",
      "Content: AT40   58 6  of stocks are trading above their respective 40 day moving averages  DMAs AT200   58 5  of stocks are trading above their respective 200DMAsVIX   10 7  volatility index   intraday high of 12 11 Short term Trading Call  cautiously bullish\n",
      "Commentary It seemed poetic  The Golden State Warriors  hailing from the tech heavy San Francisco Bay Area  were beaten suddenly  thoroughly  abruptly  and relentlessly off a sky high sugar high  a very convincing 3 games to none lead in the 2017 NBA finals   So were tech stocks \n",
      "\n",
      "The NASDAQ took a serious shellacking just one day after confirming a bullish undercurrent for the market with another all time high \n",
      "While the carnage in big cap tech stocks was painfully clear the PowerShares QQQ ETF  NASDAQ QQQ  declined 2 5  and the NASDAQ Composite declined 1 8  most of the remaining stock universe was blissfully unaware of the pain \n",
      "My favorite technical indicator  AT40  T2108   the percentage of stocks trading above their respective 40 day moving averages  DMAs   says it all  AT40 increased a solid 6 5 percentage points from 52 1  to 58 6  \n",
      "AT40  T2108  ignored the carnage in big cap tech and soared to a new 5 week high  AT40 looks much healthier now \n",
      "Even AT200  T2107   the percentage of stocks trading above their respective 200DMAs  confirmed the rally in most everything else not related to big cap tech  T2107 gained 3 percentage points to close at 58 5  \n",
      "AT200  T2107  is on the edge of erasing the previous breakdown \n",
      "The S P 500  SPDR S P 500  NYSE SPY   did its part to offset tech weakness by holding its ground with a flat close  The index even gave up an opening rally to a new all time  At the lows  the S P 500 erased all the gains from last week s breakout  It was a wild swing \n",
      "The S P 500  SPY  wavered from a new all time high to a full reversal of last week s breakout before settling at flat on the day \n",
      "Given the divergence between tech and the rest of the market  I decided to take a look at the PowerShares S P 500 Low Volatility ETF  NYSE SPLV  versus the PowerShares S P 500 High Beta ETF  NYSE SPHB   I was quite surprised to see that the High Beta crew has struggled all year to regain traction \n",
      "Meanwhile  SPLV broke out to a new all time high on May 22 and has rallied ever since  In previous posts on these ETFs  I have argued that a rally built on the success of the relatively sedate SPLV alone is a short lived one  Clearly that is not the case here \n",
      "The PowerShares S P 500 High Beta ETF  SPHB  has gone absolutely nowhere this year \n",
      "\n",
      " While the PowerShares S P 500 Low Volatility ETF  SPLV  has enjoyed a particularly strong 2017 with two big breakouts \n",
      "While the S P 500 performed relatively much better than the NASDAQ  the source of market bullishness came from smaller stocks and specific sectors  The strength in the market showed up in several indices  small caps  NYSE IWM   mid caps  NYSE MDY   even retailers  NYSE XRT   Most importantly  the financials  NYSE XLF  broke out and invalidated the bearish head and shoulders  H S  pattern I have been watching  which continues a rich tradition of H S failures  \n",
      "The iShares Russell 2000  IWM  hit a  marginal  new all time high  IWM is on the verge of confirming a very bullish breakout from an entire 2017 of bouncing around a trading range \n",
      "\n",
      "The SPDR S P MidCap 400 ETF  MDY  is like IWM except its trading range has had an ever so slight upward tilt \n",
      "\n",
      "The SPDR S P Retail ETF  XRT  soaked up the interest of value shoppers  It rallied off recent lows with a 1 4  gain \n",
      "\n",
      "The Financial Select Sector SPDR ETF  XLF  has suddenly come back to life  XLF closed the week with a breakout that invalidated the looming head and shoulders topping pattern \n",
      "Excluding the retailers  these charts stand in stark bullish contrast to the near implosion by big cap tech stocks \n",
      "Even biotech stocks held their own at the close  albeit off a one month intraday high \n",
      "The iShares Nasdaq Biotechnology  NASDAQ IBB  held above 50DMA support after it faded off a one month high \n",
      "The volatility index  the VIX  provided the icing on the cake  The sudden implosion of big cap tech stocks put enough fear in the market to send the VIX soaring all the way to 12 1  a 14 2  gain  The irony of the sudden burst is that it made my fistful of call options on ProShares Ultra VIX Short Term Futures  NYSE UVXY  quite profitable the day AFTER I thought I needed them most \n",
      "Per my strategy  I sold quickly into the spike  True to form  volatility imploded almost as quickly as it exploded  After the dust settled  the VIX closed with a much smaller gain of 5 3  and UVXY closed with a very sharp fade to a paltry  and under performing  2 8  gain \n",
      "The ProShares Ultra VIX Short Term Futures  UVXY  experienced another brief bout of glory \n",
      "\n",
      "The sharp implosion of the VIX off its high confirmed the generally bullish tone of the day  The close is right back in extremely low territory  below 11  \n",
      "The declines in the big cap tech stocks were truly stunning  Here is a sample with a listing of the closing loss and the loss at the low of the day  Netflix  NASDAQ NFLX   4 7   7 0   Alphabet  NASDAQ GOOGL   3 4   4 8   Amazon com  NASDAQ AMZN   3 2   8 2   Facebook  NASDAQ FB   3 3   5 2   Apple  NASDAQ AAPL   3 9   5 8   These losses represented some SERIOUS profit taking that surely helped fund rallies in the rest of the stock market \n",
      "AAPL had some stock specific news that exacerbated its woes  From  \n",
      "\n",
      " Verizon Corp  NYSE VZ   AT T Inc  NYSE T   and the rest of the U S  wireless industry have a big boast for this year s crop of smartphones  thanks to network upgrades  devices will be able to download as much as a gigabit of data in a single second   speeds 100 times faster than before \n",
      "But that won t be the case for Apple Inc  s  NASDAQ AAPL  newest iPhones  devices to go on sale later this year  leaving the company s most important product potentially lagging behind the data performance of rival smartphones \n",
      "The reason stems from the delicate and sometimes complicated way Apple manages the supply of the components embedded in its flagship device   in this case  the modems  which handle the connection between a phone and the cellular network  One of Apple s suppliers  Qualcomm inc   NASDAQ QCOM   sells a modem capable of the 1 gigabit download speeds  Another supplier  Intel Corp   NASDAQ INTC   is working on a modem with the same capability  but it won t be ready for the iPhone s introduction  according to people familiar with Apple s decision  \n",
      "\n",
      "Even so  I think AAPL s big loss on the day was largely in sympathy with its big cap cousins  I bought AAPL weekly call options on the initial weakness with what I thought at that time was a low ball offer  I doubled down as Apple doubled its loss on the day \n",
      "Apple  AAPL  managed to bounce off its intraday low to close right above its 50DMA   a line that has held as support since December  2016 \n",
      "As I stated in my last Above the 40 post  link above   I was poised to get aggressively bullish on the stock market if XLF  financials  invalidated the H S topping pattern  I did not count on a big piece of the market failing to cooperate \n",
      "The plunge in tech was damaging enough to keep my short term trading call at cautiously bullish  I think another major buying opportunity is coming for big cap tech stocks  but traders should exercise enough patience to let this apparent rotation of funds work itself out  A potential catalyst could be the Federal Reserve meeting on Wednesday June 14th  I imagine the rally in financials will peak around that time  Subsequently  traders should return their attention to tech stocks which will show signs of bottoming out from whatever selling pressure remains \n",
      "Still  it will be hard to get re excited about tech stocks like Nvidia  NASDAQ NVDA  which rallied to a fresh all time high only to plunge below the low of the previous day   a classic bearish engulfing top  At least buyers rallied NVDA off its lows and back to the start of the previous day s gap up \n",
      "I suspect from here NVDA will take time to heal  NVDA lost 6 5  on the day but was down as much as 10 7    yep  in one day \n",
      "Nvidia  NVDA  printed a classic bearish engulfing top \n",
      "Let s see whether the Golden State Warriors are ready to lead the charge for tech stocks to bounce back from a wicked day of selling  The sweep broom was broken but not the trend \n",
      "Daily AT40  T2108 \n",
      "Black line  AT40  T2108     measured on the right  Red line  Overbought threshold  70    Blue line  Oversold threshold  20  \n",
      "Weekly AT40  T2108 \n",
      "Be careful out there \n",
      "Full disclosure  Long AAPL call options  long NVDA call and put spread and shares\n",
      "Sentiment: Positive\n",
      "\n",
      "Sample 13488:\n",
      "Content: Here s today s swing trading watch list \n",
      "Long  Apple  O AAPL \n",
      "\n",
      "Long  Becton Dickinson  N BDX \n",
      "\n",
      "Long  Baidu  O BIDU \n",
      "\n",
      "Short  Johnson Controls  N JCI \n",
      "\n",
      "Short  LinkedIn  N LNKD \n",
      "Sentiment: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adjust display settings for better visualization of samples\n",
    "pd.set_option('display.max_colwidth', 200)  # Adjust the width to fit longer texts\n",
    "\n",
    "# Display some random samples with formatted output\n",
    "sample_data = data.sample(n=10)[['content', 'sentiment']]\n",
    "\n",
    "# Print each sample in a more readable format\n",
    "for index, row in sample_data.iterrows():\n",
    "    print(f\"Sample {index}:\")\n",
    "    print(f\"Content: {row['content']}\")\n",
    "    print(f\"Sentiment: {row['sentiment']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Distribution (%):\n",
      "sentiment\n",
      "Positive    47.453805\n",
      "Neutral     33.535860\n",
      "Negative    19.010335\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Label Distribution (%):\n",
      "label\n",
      "1    55.34607\n",
      "0    44.65393\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Cross-Tabulation of Sentiment and Label (%):\n",
      "label              0          1\n",
      "sentiment                      \n",
      "Negative   50.016474  49.983526\n",
      "Neutral    46.413896  53.586104\n",
      "Positive   41.261880  58.738120\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'data' is your DataFrame with 'sentiment' and 'label' columns\n",
    "# Calculate the proportion of each sentiment category\n",
    "sentiment_counts = data['sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Calculate the proportion of each label\n",
    "label_counts = data['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Sentiment Distribution (%):\")\n",
    "print(sentiment_counts)\n",
    "print(\"\\nLabel Distribution (%):\")\n",
    "print(label_counts)\n",
    "\n",
    "# For additional insights, we can also look at the cross-tabulation of sentiment and label\n",
    "crosstab = pd.crosstab(data['sentiment'], data['label'], normalize='index') * 100\n",
    "print(\"\\nCross-Tabulation of Sentiment and Label (%):\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read for existed csv\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./data/dataset_with_sentiment.csv')\n",
    "# sliding window\n",
    "# TODO\n",
    "\n",
    "# Convert the 'Date' column to datetime format and sort the dataframe by 'Date'\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data_sorted = data.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按 'Date' 和 'sentiment' 分组，然后计算每个类别的 category 为news和opinion的数量\n",
    "category_news_per_day_sentiment = data_sorted[data_sorted['category'] == 'news'].groupby(['Date', 'sentiment']).size().unstack().fillna(0)\n",
    "category_opinion_per_day_sentiment = data_sorted[data_sorted['category'] == 'opinion'].groupby(['Date', 'sentiment']).size().unstack().fillna(0)\n",
    "# 分别计算news和opinion的total\n",
    "category_news_total_per_day_sentiment = data_sorted[data_sorted['category'] == 'news'].groupby(['Date']).size()\n",
    "category_opinion_total_per_day_sentiment = data_sorted[data_sorted['category'] == 'opinion'].groupby(['Date']).size()\n",
    "\n",
    "data_sorted = data_sorted.set_index('Date')\n",
    "data_sorted['P_news_pos'] = category_news_per_day_sentiment['Positive'].reindex(data_sorted.index) / category_news_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_news_neg'] = category_news_per_day_sentiment['Negative'].reindex(data_sorted.index) / category_news_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_op_pos'] = category_opinion_per_day_sentiment['Positive'].reindex(data_sorted.index) / category_opinion_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_op_neg'] = category_opinion_per_day_sentiment['Negative'].reindex(data_sorted.index) / category_opinion_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted = data_sorted.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data = data_sorted.groupby('Date').last()\n",
    "\n",
    "# Shift the 'Open' column to get the next day's opening price\n",
    "daily_data['Next_Open'] = daily_data['Open'].shift(-1)\n",
    "\n",
    "# Drop the last row as it will not have a 'Next_Open' value\n",
    "daily_data = daily_data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2016-10-28')\n",
    "daily_data.loc[(date_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2016-10-28')\n",
    "data_sorted.loc[data_sorted['Date'] == date_to_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2020-01-23')\n",
    "daily_data.loc[(date_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2018-05-06')\n",
    "category_to_query = 'news'\n",
    "data_sorted.loc[(data_sorted['Date'] == date_to_query) & (data_sorted['category'] == category_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data['P_news_neg'].fillna(0, inplace=True)\n",
    "daily_data['P_news_pos'].fillna(0, inplace=True)\n",
    "daily_data['P_op_neg'].fillna(0, inplace=True)\n",
    "daily_data['P_op_pos'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the dataset into a Pandas DataFrame\n",
    "historical_data = pd.read_csv('data/AAPL_Yahoo_Correct.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "\n",
    "# Plotting the 'Open' price against the 'Date'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(historical_data['Date'], historical_data['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Date将historical_data的全部列和daily_data的这四个P_开头的列合并。如果出现有些天在daily_data中不存在，则四个P_开头的列在这一天都置为0。\n",
    "daily_data_merged = pd.merge(historical_data, daily_data[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']], left_on='Date', right_index=True, how='left')\n",
    "daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']] = daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(daily_data_merged['Date'], daily_data_merged['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.to_csv('./data/dataset_FinBERT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.1: VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the dataset into a Pandas DataFrame\n",
    "historical_data = pd.read_csv('data/AAPL_Yahoo_Correct.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "\n",
    "# Plotting the 'Open' price against the 'Date'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(historical_data['Date'], historical_data['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read VADER results from csv\n",
    "# vader_daily_results = pd.read_csv('./data/VADER_results.csv')\n",
    "# vader_daily_results = pd.read_csv('./data/combined_data_mean5.csv')\n",
    "# vader_daily_results = pd.read_csv('./data/combined_data_mean_first512.csv')\n",
    "vader_daily_results = pd.read_csv('./data/proportion_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_daily_results['Date'] = pd.to_datetime(vader_daily_results['Date'])\n",
    "vader_daily_results = vader_daily_results[['Date', 'news_neg', 'news_pos', 'opinion_neg', 'opinion_pos']]\n",
    "vader_daily_results.columns = ['Date', 'P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']\n",
    "vader_daily_results.set_index('Date', inplace=True)\n",
    "\n",
    "daily_data_merged = pd.merge(historical_data, vader_daily_results[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']], left_on='Date', right_index=True, how='left')\n",
    "daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']] = daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.to_csv('./data/dataset_VADER.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Stock price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# FinBERT\n",
    "daily_data_merged = pd.read_csv('./data/dataset_FinBERT.csv', index_col=0)\n",
    "# VADER\n",
    "# daily_data_merged = pd.read_csv('./data/dataset_VADER.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Showing sentiment analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment_results = daily_data_merged[['Date', 'P_news_pos', 'P_news_neg', 'P_op_pos', 'P_op_neg']]\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the data into two parts: news sentiment analysis results and opinion sentiment analysis results\n",
    "news_sentiment_results = daily_sentiment_results[['Date', 'P_news_pos', 'P_news_neg']]\n",
    "opinion_sentiment_results = daily_sentiment_results[['Date', 'P_op_pos', 'P_op_neg']]\n",
    "\n",
    "# Ensure 'Date' is in news_sentiment_results and opinion_sentiment_results\n",
    "assert 'Date' in news_sentiment_results.columns\n",
    "assert 'Date' in opinion_sentiment_results.columns\n",
    "\n",
    "# Set 'Date' column as index\n",
    "news_sentiment_results.set_index('Date', inplace=True)\n",
    "opinion_sentiment_results.set_index('Date', inplace=True)\n",
    "\n",
    "# Draw a heatmap for news sentiment analysis results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('News Sentiment Analysis Results')\n",
    "sns.heatmap(news_sentiment_results.tail(5), annot=True, cmap='YlGnBu', fmt=\".3f\")\n",
    "plt.show()\n",
    "\n",
    "# Draw a heatmap for opinion sentiment analysis results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Opinion Sentiment Analysis Results')\n",
    "sns.heatmap(opinion_sentiment_results.tail(5), annot=True, cmap='YlGnBu', fmt=\".3f\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择特征和目标\n",
    "# 保留原本的index，将'Date'列单独提取出来保存\n",
    "date = daily_data_merged['Date']\n",
    "date = pd.to_datetime(date)\n",
    "\n",
    "features = daily_data_merged.drop(['Date'], axis=1)\n",
    "# Open作为预测目标\n",
    "target = daily_data_merged['Open']\n",
    "features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply the MinMaxScaler to the features and target\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# fit_transform根据数据计算缩放参数\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# 保存缩放参数\n",
    "import joblib\n",
    "joblib.dump(scaler_features, './model/scaler_features.pkl')\n",
    "joblib.dump(scaler_target, './model/scaler_target.pkl')\n",
    "\n",
    "# Create new DataFrames with the scaled features and target\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "scaled_target_df = pd.DataFrame(scaled_target, columns=['Open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features, targets, seq_length):\n",
    "    \"\"\"\n",
    "    Create sequences of specified length from time series data.\n",
    "\n",
    "    Args:\n",
    "    features (np.array): The feature data.\n",
    "    targets (np.array): The target data.\n",
    "    seq_length (int): The length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Sequences of features.\n",
    "    np.array: Corresponding targets for each sequence.\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        x = features[i:(i + seq_length)]\n",
    "        y = targets[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length\n",
    "seq_length = 25\n",
    "\n",
    "# Create sequences\n",
    "features_seq, target_seq = create_sequences(scaled_features, scaled_target, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "    features_seq, target_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_features, test_features, val_target, test_target = train_test_split(\n",
    "    test_features, test_target, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences to Tensor\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_target = torch.tensor(train_target, dtype=torch.float32)\n",
    "\n",
    "val_features = torch.tensor(val_features, dtype=torch.float32)\n",
    "val_target = torch.tensor(val_target, dtype=torch.float32)\n",
    "\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_target = torch.tensor(test_target, dtype=torch.float32)\n",
    "\n",
    "# 创建TensorDataset\n",
    "train_dataset = TensorDataset(train_features, train_target)\n",
    "val_dataset = TensorDataset(val_features, val_target)\n",
    "test_dataset = TensorDataset(test_features, test_target)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用普通的LSTM模型，不使用注意力机制\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_layers, output_dim, dropout=0.2):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 打印lstm_out的形状\n",
    "        # print(lstm_out.shape)\n",
    "        # 取最后一个时间步的输出\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用注意力机制的LSTM\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.attn = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         attn_weights = torch.tanh(self.attn(encoder_outputs))\n",
    "#         return torch.bmm(attn_weights.transpose(1, 2), encoder_outputs).squeeze(1)\n",
    "\n",
    "# class AttentionLSTM(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_size, num_layers, output_dim, dropout=0.2):\n",
    "#         super(AttentionLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         # LSTM层\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_size, num_layers, \n",
    "#                             batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "#         # 注意力层\n",
    "#         self.attention = Attention(hidden_size)\n",
    "        \n",
    "#         # 全连接层\n",
    "#         self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         attn_out = self.attention(lstm_out[:, -1, :], lstm_out)\n",
    "#         output = self.fc(attn_out)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "input_dim = scaled_features_df.shape[1]  # 特征数量\n",
    "hidden_size = 100  # 隐藏状态中的特征数量，可以调整\n",
    "num_layers = 4    # 堆叠的LSTM层的数量\n",
    "output_dim = 1    # 输出维度的数量（预测一个值）\n",
    "\n",
    "# 使用SimpleLSTM\n",
    "model = SimpleLSTM(input_dim, hidden_size, num_layers, output_dim, dropout=0.2)\n",
    "# 使用AttentionLSTM\n",
    "# model = AttentionLSTM(input_dim, hidden_size, num_layers, output_dim, dropout=0.2)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './model/best_model.pth')\n",
    "        print('best_model updated at epoch {}, best_val_loss : {:.4f}'.format(epoch+1, best_val_loss))\n",
    "        \n",
    "    # 每5轮打印一次train loss和val loss\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # 记录两个loss\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "# 在所有epochs结束后绘制损失图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_epochs), train_loss_list, label='Train Loss', color='blue')\n",
    "plt.plot(range(num_epochs), val_loss_list, label='Validation Loss', color='red')\n",
    "plt.title('Train Loss and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load('./model/best_model.pth'))\n",
    "\n",
    "# 计算新的测试集的大小\n",
    "test_size_new = int(len(features_seq) * 0.05)\n",
    "\n",
    "# 按时间顺序划分新的测试集\n",
    "test_features_new, test_target_new = features_seq[-test_size_new:], target_seq[-test_size_new:]\n",
    "\n",
    "# 使用模型进行预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions_new = model(torch.tensor(test_features_new, dtype=torch.float32)).numpy()\n",
    "\n",
    "# 反缩放预测值\n",
    "test_predictions_new = scaler_target.inverse_transform(test_predictions_new).flatten()\n",
    "\n",
    "# 反缩放真实目标值\n",
    "test_target_new = scaler_target.inverse_transform(test_target_new.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 计算日期的总长度\n",
    "total_length = len(date)\n",
    "\n",
    "# 计算测试集的开始位置\n",
    "test_start = total_length - test_size_new\n",
    "\n",
    "# 计算新的测试集的结束位置\n",
    "test_end = total_length\n",
    "\n",
    "# 获取新的测试集的日期范围\n",
    "test_date_new = date[test_start:test_end]\n",
    "\n",
    "# Print the date range of the new test set\n",
    "print(\"The date range of the new test set is from\", test_date_new.iloc[0], \"to\", test_date_new.iloc[-1])\n",
    "\n",
    "# Print the length of the new test set\n",
    "print(\"The length is\", len(test_target_new))\n",
    "\n",
    "# 绘制实际股价和预测股价的对比图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(date[test_start:], test_target_new, label='Actual Prices', color='blue')\n",
    "plt.plot(date[test_start:], test_predictions_new, label='Predicted Prices', color='red')\n",
    "\n",
    "# 设置x轴的日期格式\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=10))  # 设置日期间隔\n",
    "\n",
    "plt.title('Predicted vs Actual Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.gcf().autofmt_xdate()  # 自动调整x轴日期标签的角度以提高可读性\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 计算MSE\n",
    "mse = mean_squared_error(test_target_new, test_predictions_new)\n",
    "print('Test MSE: ', mse)\n",
    "# 计算RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print('Test RMSE: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 50个epoch RMSE测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建一个数据框来显示序列长度和对应的RMSE\n",
    "seq_rmse_df = pd.DataFrame({\n",
    "    'Seq_length': [10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'RMSE': [1.1130488933158105, 0.8456300018583505, 0.8641183126757913, 0.8094936259619585, 0.9580832258849209, 0.8409732222581049, 0.9262937021912081, 1.1064503987204368, 0.9005207820679706]\n",
    "})\n",
    "\n",
    "# 将RMSE保留两位小数\n",
    "seq_rmse_df['RMSE'] = seq_rmse_df['RMSE'].round(3)\n",
    "seq_rmse_df = seq_rmse_df.reset_index(drop=True)\n",
    "\n",
    "import matplotlib.cm\n",
    "from plottable import ColumnDefinition, Table\n",
    "\n",
    "# 创建列定义\n",
    "seq_length_col_def = ColumnDefinition('Seq_length', title='sequence length')\n",
    "rmse_col_def = ColumnDefinition('RMSE', title='RMSE', cmap=matplotlib.cm.get_cmap('viridis'))\n",
    "\n",
    "# 创建表格\n",
    "table = Table(seq_rmse_df, column_definitions=[seq_length_col_def, rmse_col_def])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "rmse_results = [1.1130488933158105, 0.8456300018583505, 0.8641183126757913, 0.8094936259619585, 0.9580832258849209, 0.8409732222581049, 0.9262937021912081, 1.1064503987204368, 0.9005207820679706]\n",
    "\n",
    "# 绘制RMSE结果图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, rmse_results, label='RMSE Results', color='blue')\n",
    "\n",
    "# 标出最小RMSE点\n",
    "min_rmse_index = np.argmin(rmse_results)\n",
    "plt.plot(seq_lengths[min_rmse_index], rmse_results[min_rmse_index], 'ro')\n",
    "plt.text(seq_lengths[min_rmse_index], rmse_results[min_rmse_index], f'Min RMSE: {rmse_results[min_rmse_index]:.3f}', fontsize=12, ha='right')\n",
    "\n",
    "plt.title('RMSE Results for Different Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_data = {\n",
    "    'Method': ['Historical transaction information', 'FinBERT mixed', 'VADER mixed'],\n",
    "    'RMSE': [0.9793, 0.7341, 0.8601]\n",
    "}\n",
    "rmse_df = pd.DataFrame(rmse_data)\n",
    "\n",
    "# 设置表格样式\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 创建一个新的figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 使用seaborn的barplot函数绘制条形图\n",
    "sns.barplot(x='Method', y='RMSE', data=rmse_df, palette='viridis')\n",
    "\n",
    "# 设置标题和坐标轴标签\n",
    "plt.title('RMSE Results for Different Methods')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
