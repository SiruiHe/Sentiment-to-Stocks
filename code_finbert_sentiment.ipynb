{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Apple Set To Beat Q1 Earnings Estimates  Tech ...</td>\n",
       "      <td>Technology giant Apple   NASDAQ AAPL   is set ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Tech Daily  Intel Results  Netflix Surge  Appl...</td>\n",
       "      <td>The top stories in this digest are Intel s   N...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>7 Monster Stock Market Predictions For The Wee...</td>\n",
       "      <td>S P 500  SPY \\nThis week will be packed with e...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Apple Earnings Preview  5G Launch  Expanding S...</td>\n",
       "      <td>Reports Q1 2020 results on Tuesday  Jan  28 ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Buy Surging Apple   Microsoft Stock Before Qua...</td>\n",
       "      <td>On today s episode of Full Court Finance here ...</td>\n",
       "      <td>77.514999</td>\n",
       "      <td>77.942497</td>\n",
       "      <td>76.220001</td>\n",
       "      <td>77.237503</td>\n",
       "      <td>75.793358</td>\n",
       "      <td>161940000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 ticker        Date category  \\\n",
       "0           0   AAPL  2020-01-27  opinion   \n",
       "1           1   AAPL  2020-01-27  opinion   \n",
       "2           2   AAPL  2020-01-27  opinion   \n",
       "3           3   AAPL  2020-01-27  opinion   \n",
       "4           4   AAPL  2020-01-27  opinion   \n",
       "\n",
       "                                               title  \\\n",
       "0  Apple Set To Beat Q1 Earnings Estimates  Tech ...   \n",
       "1  Tech Daily  Intel Results  Netflix Surge  Appl...   \n",
       "2  7 Monster Stock Market Predictions For The Wee...   \n",
       "3  Apple Earnings Preview  5G Launch  Expanding S...   \n",
       "4  Buy Surging Apple   Microsoft Stock Before Qua...   \n",
       "\n",
       "                                             content       Open       High  \\\n",
       "0  Technology giant Apple   NASDAQ AAPL   is set ...  77.514999  77.942497   \n",
       "1  The top stories in this digest are Intel s   N...  77.514999  77.942497   \n",
       "2  S P 500  SPY \\nThis week will be packed with e...  77.514999  77.942497   \n",
       "3    Reports Q1 2020 results on Tuesday  Jan  28 ...  77.514999  77.942497   \n",
       "4  On today s episode of Full Court Finance here ...  77.514999  77.942497   \n",
       "\n",
       "         Low      Close  Adj Close     Volume  label  \n",
       "0  76.220001  77.237503  75.793358  161940000      0  \n",
       "1  76.220001  77.237503  75.793358  161940000      0  \n",
       "2  76.220001  77.237503  75.793358  161940000      0  \n",
       "3  76.220001  77.237503  75.793358  161940000      0  \n",
       "4  76.220001  77.237503  75.793358  161940000      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './data/NEWS_YAHOO_stock_prediction.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Technology giant Apple   NASDAQ AAPL   is set ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>The top stories in this digest are Intel s   N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>S P 500  SPY \\nThis week will be packed with e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Reports Q1 2020 results on Tuesday  Jan  28 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>On today s episode of Full Court Finance here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>opinion</td>\n",
       "      <td>Monday  January 27  2020The Zacks Research Dai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>news</td>\n",
       "      <td>By Peter Nurse \\nInvesting com   European stoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>news</td>\n",
       "      <td>BTIG analyst Mark Palmer initiates coverage of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>opinion</td>\n",
       "      <td>I got a great question recently from Barbara P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>opinion</td>\n",
       "      <td>We get into the heart of the Q4 earnings seaso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date category                                            content\n",
       "0  2020-01-27  opinion  Technology giant Apple   NASDAQ AAPL   is set ...\n",
       "1  2020-01-27  opinion  The top stories in this digest are Intel s   N...\n",
       "2  2020-01-27  opinion  S P 500  SPY \\nThis week will be packed with e...\n",
       "3  2020-01-27  opinion    Reports Q1 2020 results on Tuesday  Jan  28 ...\n",
       "4  2020-01-27  opinion  On today s episode of Full Court Finance here ...\n",
       "5  2020-01-27  opinion  Monday  January 27  2020The Zacks Research Dai...\n",
       "6  2020-01-27     news  By Peter Nurse \\nInvesting com   European stoc...\n",
       "7  2020-01-27     news  BTIG analyst Mark Palmer initiates coverage of...\n",
       "8  2020-01-24  opinion  I got a great question recently from Barbara P...\n",
       "9  2020-01-24  opinion  We get into the heart of the Q4 earnings seaso..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data[['Date', 'category', 'content']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15965 entries, 0 to 15974\n",
      "Data columns (total 12 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ticker     15965 non-null  object \n",
      " 1   Date       15965 non-null  object \n",
      " 2   category   15965 non-null  object \n",
      " 3   title      15965 non-null  object \n",
      " 4   content    15965 non-null  object \n",
      " 5   Open       15965 non-null  float64\n",
      " 6   High       15965 non-null  float64\n",
      " 7   Low        15965 non-null  float64\n",
      " 8   Close      15965 non-null  float64\n",
      " 9   Adj Close  15965 non-null  float64\n",
      " 10  Volume     15965 non-null  int64  \n",
      " 11  label      15965 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>15965.000000</td>\n",
       "      <td>1.596500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.583061</td>\n",
       "      <td>40.952148</td>\n",
       "      <td>40.241173</td>\n",
       "      <td>40.605005</td>\n",
       "      <td>38.739098</td>\n",
       "      <td>1.536463e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.884583</td>\n",
       "      <td>11.980327</td>\n",
       "      <td>11.799389</td>\n",
       "      <td>11.891820</td>\n",
       "      <td>12.158320</td>\n",
       "      <td>1.096033e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.856071</td>\n",
       "      <td>14.271429</td>\n",
       "      <td>13.753571</td>\n",
       "      <td>13.947500</td>\n",
       "      <td>12.084597</td>\n",
       "      <td>4.544800e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.522499</td>\n",
       "      <td>31.772499</td>\n",
       "      <td>31.264999</td>\n",
       "      <td>31.475000</td>\n",
       "      <td>28.576729</td>\n",
       "      <td>9.517400e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.937500</td>\n",
       "      <td>41.432499</td>\n",
       "      <td>40.602501</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.263371</td>\n",
       "      <td>1.211508e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.125000</td>\n",
       "      <td>47.424999</td>\n",
       "      <td>46.695000</td>\n",
       "      <td>47.037498</td>\n",
       "      <td>45.263882</td>\n",
       "      <td>1.691264e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.062500</td>\n",
       "      <td>80.832497</td>\n",
       "      <td>79.379997</td>\n",
       "      <td>79.807503</td>\n",
       "      <td>78.315315</td>\n",
       "      <td>1.460852e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open          High           Low         Close     Adj Close  \\\n",
       "count  15965.000000  15965.000000  15965.000000  15965.000000  15965.000000   \n",
       "mean      40.583061     40.952148     40.241173     40.605005     38.739098   \n",
       "std       11.884583     11.980327     11.799389     11.891820     12.158320   \n",
       "min       13.856071     14.271429     13.753571     13.947500     12.084597   \n",
       "25%       31.522499     31.772499     31.264999     31.475000     28.576729   \n",
       "50%       40.937500     41.432499     40.602501     41.000000     39.263371   \n",
       "75%       47.125000     47.424999     46.695000     47.037498     45.263882   \n",
       "max       80.062500     80.832497     79.379997     79.807503     78.315315   \n",
       "\n",
       "             Volume  \n",
       "count  1.596500e+04  \n",
       "mean   1.536463e+08  \n",
       "std    1.096033e+08  \n",
       "min    4.544800e+07  \n",
       "25%    9.517400e+07  \n",
       "50%    1.211508e+08  \n",
       "75%    1.691264e+08  \n",
       "max    1.460852e+09  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Remove unnecessary column\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Step 2: Remove duplicate texts\n",
    "data.drop_duplicates(subset=['title', 'content'], inplace=True)\n",
    "\n",
    "# Step 3: Remove rows with large amount of spaces or empty texts in 'title' and 'content'\n",
    "data = data[~data['title'].str.isspace()]\n",
    "data = data[~data['content'].str.isspace()]\n",
    "data.dropna(subset=['title', 'content'], inplace=True)\n",
    "\n",
    "# Check the dataframe after these preprocessing steps\n",
    "data.info()\n",
    "\n",
    "# Step 5: Check for invalid numeric data\n",
    "numeric_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "data[numeric_columns].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) set proxy\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source ~/clash_dir/set && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "output\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the FinBERT model and tokenizer\n",
    "checkpoint = 'yiyanghkust/finbert-tone'\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "# Create a pipeline for sentiment analysis\n",
    "# Do not truncate the original text\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑动窗口方法\n",
    "from collections import Counter\n",
    "\n",
    "def sliding_window(text, max_len, overlap):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    text (str): the text to be split into chunks\n",
    "    max_len (int): the maximum length of each chunk\n",
    "    overlap (int): the number of overlapped tokens between chunks\n",
    "\n",
    "    Returns:\n",
    "    list of str: the list of text chunks\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_len - overlap):\n",
    "        chunk = tokens[i:i + max_len]\n",
    "        chunk = tokenizer.convert_tokens_to_string(chunk)\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# 普通滑动窗口\n",
    "# def apply_sentiment_analysis(df, nlp, text_column='content', max_len=512, overlap=50):\n",
    "#     \"\"\"\n",
    "#     Apply sentiment analysis to a column in a dataframe using sliding window method.\n",
    "    \n",
    "#     Args:\n",
    "#     df (pd.DataFrame): Dataframe containing the text data.\n",
    "#     nlp (pipeline): HuggingFace pipeline for sentiment analysis.\n",
    "#     text_column (str): Name of the column containing text data.\n",
    "#     max_len (int): The maximum length of each text chunk.\n",
    "#     overlap (int): The number of overlapped tokens between chunks.\n",
    "\n",
    "#     Returns:\n",
    "#     pd.DataFrame: Dataframe with a new column 'sentiment' containing the analysis results.\n",
    "#     \"\"\"\n",
    "#     sentiments = []\n",
    "#     for text in df[text_column]:\n",
    "#         try:\n",
    "#             # Apply sliding window to the text\n",
    "#             text_chunks = sliding_window(text, max_len, overlap)\n",
    "\n",
    "#             # Apply sentiment analysis to each chunk\n",
    "#             chunk_sentiments = []\n",
    "#             for chunk in text_chunks:\n",
    "#                 result = nlp(chunk)\n",
    "#                 chunk_sentiments.append(result[0]['label'])\n",
    "\n",
    "#             # Combine the sentiments\n",
    "#             # Here we simply take the most common sentiment as the final sentiment\n",
    "#             final_sentiment = Counter(chunk_sentiments).most_common(1)[0][0]\n",
    "#             sentiments.append(final_sentiment)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in processing text: {e}\")\n",
    "#             sentiments.append('Error')\n",
    "\n",
    "#     # Add the sentiments as a new column in the dataframe\n",
    "#     df['sentiment'] = sentiments\n",
    "#     return df\n",
    "\n",
    "# 加权投票\n",
    "def apply_sentiment_analysis(df, nlp, text_column='content', max_len=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to a column in a dataframe using sliding window method.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Dataframe containing the text data.\n",
    "    nlp (pipeline): HuggingFace pipeline for sentiment analysis.\n",
    "    text_column (str): Name of the column containing text data.\n",
    "    max_len (int): The maximum length of each text chunk.\n",
    "    overlap (int): The number of overlapped tokens between chunks.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with a new column 'sentiment' containing the analysis results.\n",
    "    \"\"\"\n",
    "    sentiments = []\n",
    "    for text in df[text_column]:\n",
    "        try:\n",
    "            # Apply sliding window to the text\n",
    "            text_chunks = sliding_window(text, max_len, overlap)\n",
    "\n",
    "            # Apply sentiment analysis to each chunk\n",
    "            chunk_sentiments = []\n",
    "            chunk_weights = []\n",
    "            for chunk in text_chunks:\n",
    "                result = nlp(chunk)\n",
    "                sentiment = result[0]['label']\n",
    "                chunk_sentiments.append(sentiment)\n",
    "                # Use the length of the chunk as the weight\n",
    "                weight = len(chunk)\n",
    "                chunk_weights.append(weight)\n",
    "\n",
    "            # Combine the sentiments using weighted voting\n",
    "            sentiment_counter = Counter()\n",
    "            for sentiment, weight in zip(chunk_sentiments, chunk_weights):\n",
    "                sentiment_counter[sentiment] += weight\n",
    "            final_sentiment = sentiment_counter.most_common(1)[0][0]\n",
    "            sentiments.append(final_sentiment)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing text: {e}\")\n",
    "            sentiments.append('Error')\n",
    "\n",
    "    # Add the sentiments as a new column in the dataframe\n",
    "    df['sentiment'] = sentiments\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def process_batch(texts, nlp, max_len, overlap):\n",
    "    sentiments = []\n",
    "    for text in texts:\n",
    "        # Apply sliding window to the text\n",
    "        text_chunks = sliding_window(text, max_len, overlap)\n",
    "\n",
    "        # Apply sentiment analysis to each chunk\n",
    "        chunk_sentiments = []\n",
    "        chunk_weights = []\n",
    "        for chunk in text_chunks:\n",
    "            result = nlp(chunk)\n",
    "            sentiment = result[0]['label']\n",
    "            chunk_sentiments.append(sentiment)\n",
    "            # Use the length of the chunk as the weight\n",
    "            weight = len(chunk)\n",
    "            chunk_weights.append(weight)\n",
    "\n",
    "        # Combine the sentiments using weighted voting\n",
    "        sentiment_counter = Counter()\n",
    "        for sentiment, weight in zip(chunk_sentiments, chunk_weights):\n",
    "            sentiment_counter[sentiment] += weight\n",
    "        final_sentiment = sentiment_counter.most_common(1)[0][0]\n",
    "        sentiments.append(final_sentiment)\n",
    "    return sentiments\n",
    "\n",
    "def apply_sentiment_analysis_parallel(df, nlp, text_column='content', max_len=500, overlap=50, num_workers=16, batch_size=10):\n",
    "    \"\"\"\n",
    "    Apply sentiment analysis to a column in a dataframe using sliding window method.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Dataframe containing the text data.\n",
    "    nlp (pipeline): HuggingFace pipeline for sentiment analysis.\n",
    "    text_column (str): Name of the column containing text data.\n",
    "    max_len (int): The maximum length of each text chunk.\n",
    "    overlap (int): The number of overlapped tokens between chunks.\n",
    "    num_workers (int): The number of threads to use for parallel processing.\n",
    "    batch_size (int): The number of texts to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with a new column 'sentiment' containing the analysis results.\n",
    "    \"\"\"\n",
    "    # Break the texts into batches\n",
    "    text_batches = [df[text_column][i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    sentiments = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_batch, batch, nlp, max_len, overlap) for batch in text_batches]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            try:\n",
    "                sentiments.extend(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Error in processing text: {e}\")\n",
    "                sentiments.extend(['Error'] * batch_size)\n",
    "\n",
    "    # Add the sentiments as a new column in the dataframe\n",
    "    df['sentiment'] = sentiments\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 直接截断前512\n",
    "# def apply_sentiment_analysis(df, nlp, text_column='content'):\n",
    "#     \"\"\"\n",
    "#     Apply sentiment analysis to a column in a dataframe.\n",
    "    \n",
    "#     Args:\n",
    "#     df (pd.DataFrame): Dataframe containing the text data.\n",
    "#     nlp (pipeline): HuggingFace pipeline for sentiment analysis.\n",
    "#     text_column (str): Name of the column containing text data.\n",
    "\n",
    "#     Returns:\n",
    "#     pd.DataFrame: Dataframe with a new column 'sentiment' containing the analysis results.\n",
    "#     \"\"\"\n",
    "#     # Apply sentiment analysis to each row in the text column\n",
    "#     sentiments = []\n",
    "#     for text in df[text_column]:\n",
    "#         try:\n",
    "#             result = nlp(text)\n",
    "#             sentiments.append(result[0]['label'])\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in processing text: {e}\")\n",
    "#             sentiments.append('Error')\n",
    "\n",
    "#     # Add the sentiments as a new column in the dataframe\n",
    "#     df['sentiment'] = sentiments\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# import numpy as np\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# def apply_sentiment_analysis_parallel(df, nlp, text_column='content', batch_size=10):\n",
    "#     \"\"\"\n",
    "#     Apply sentiment analysis in parallel to a column in a dataframe.\n",
    "\n",
    "#     Args:\n",
    "#     df (pd.DataFrame): Dataframe containing the text data.\n",
    "#     nlp (pipeline): HuggingFace pipeline for sentiment analysis.\n",
    "#     text_column (str): Name of the column containing text data.\n",
    "#     batch_size (int): Number of texts to process in parallel.\n",
    "\n",
    "#     Returns:\n",
    "#     pd.DataFrame: Dataframe with a new column 'sentiment' containing the analysis results.\n",
    "#     \"\"\"\n",
    "#     # Define a function to process a batch of texts\n",
    "#     def process_batch(texts):\n",
    "#         return [nlp(text)[0]['label'] for text in texts]\n",
    "\n",
    "#     # Break the texts into batches\n",
    "#     batches = [df[text_column][i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "#     # Process batches in parallel\n",
    "#     sentiments = []\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         for batch_result in tqdm(executor.map(process_batch, batches), total=len(batches)):\n",
    "#             sentiments.extend(batch_result)\n",
    "\n",
    "#     # Add the sentiments as a new column in the dataframe\n",
    "#     df['sentiment'] = sentiments\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've been waiting for a HuggingFace course my ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So have I!</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content sentiment\n",
       "0  I've been waiting for a HuggingFace course my ...   Neutral\n",
       "1                                         So have I!   Neutral"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the function\n",
    "# Note: You will run this on your local machine as it requires GPU support\n",
    "sample_texts = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "sample_df = pd.DataFrame(sample_texts, columns=['content'])\n",
    "apply_sentiment_analysis(sample_df, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a078e85aea34023ac3edf8c2529270a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/root/Workspace/Playground/code_finbert_sentiment.ipynb 单元格 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfrom_pandas(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 创建一个新的列来存储情感分析结果\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m example: {\u001b[39m'\u001b[39;49m\u001b[39msentiment\u001b[39;49m\u001b[39m'\u001b[39;49m: nlp(example[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m])[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]}, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 将情感分析结果转换为数值\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m sentiment_mapping \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:591\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    590\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    592\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    593\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    594\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:556\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    550\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    554\u001b[0m }\n\u001b[1;32m    555\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    557\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    558\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3089\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3082\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3083\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3084\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3085\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3087\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3088\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3089\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3090\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3091\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3466\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3462\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3463\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3464\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3465\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3466\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3467\u001b[0m         batch,\n\u001b[1;32m   3468\u001b[0m         indices,\n\u001b[1;32m   3469\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3470\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3471\u001b[0m     )\n\u001b[1;32m   3472\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3473\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3474\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3475\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3345\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3343\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3344\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3345\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3347\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3348\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3349\u001b[0m     }\n",
      "\u001b[1;32m/root/Workspace/Playground/code_finbert_sentiment.ipynb 单元格 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfrom_pandas(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 创建一个新的列来存储情感分析结果\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m example: {\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m: nlp(example[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m])[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]}, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 将情感分析结果转换为数值\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-3.seetacloud.com/root/Workspace/Playground/code_finbert_sentiment.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m sentiment_mapping \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/formatting/formatting.py:270\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 270\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[key]\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys_to_format:\n\u001b[1;32m    272\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# 将你的数据转换为Dataset对象\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# 创建一个新的列来存储情感分析结果\n",
    "dataset = dataset.map(lambda example: {'sentiment': nlp(example['text'])[0]['label']}, batched=True, batch_size=16)\n",
    "\n",
    "# 将情感分析结果转换为数值\n",
    "sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "dataset = dataset.map(lambda example: {'sentiment_numeric': sentiment_mapping[example['sentiment']]})\n",
    "\n",
    "# 将处理后的数据转换回Pandas DataFrame\n",
    "df_processed = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming the apply_sentiment_analysis function is defined as shown previously\n",
    "\n",
    "# # Step 1: Apply sentiment analysis to the dataset\n",
    "# # This step should be done on your local machine due to the requirement of GPU support\n",
    "# data = apply_sentiment_analysis_parallel(data, nlp)\n",
    "\n",
    "# # Step 2: Prepare data for the prediction model\n",
    "# # Here we'll assume the sentiment analysis has been applied and 'sentiment' column is added to the data\n",
    "\n",
    "# # We might want to convert sentiments to numerical values for model training\n",
    "# sentiment_mapping = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "# data['sentiment_numeric'] = data['sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to save the processed DataFrame to a CSV file\n",
    "data.to_csv('./data/dataset_with_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust display settings for better visualization of samples\n",
    "pd.set_option('display.max_colwidth', 200)  # Adjust the width to fit longer texts\n",
    "\n",
    "# Display some random samples with formatted output\n",
    "sample_data = data.sample(n=10)[['content', 'sentiment']]\n",
    "\n",
    "# Print each sample in a more readable format\n",
    "for index, row in sample_data.iterrows():\n",
    "    print(f\"Sample {index}:\")\n",
    "    print(f\"Content: {row['content']}\")\n",
    "    print(f\"Sentiment: {row['sentiment']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame with 'sentiment' and 'label' columns\n",
    "# Calculate the proportion of each sentiment category\n",
    "sentiment_counts = data['sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Calculate the proportion of each label\n",
    "label_counts = data['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Sentiment Distribution (%):\")\n",
    "print(sentiment_counts)\n",
    "print(\"\\nLabel Distribution (%):\")\n",
    "print(label_counts)\n",
    "\n",
    "# For additional insights, we can also look at the cross-tabulation of sentiment and label\n",
    "crosstab = pd.crosstab(data['sentiment'], data['label'], normalize='index') * 100\n",
    "print(\"\\nCross-Tabulation of Sentiment and Label (%):\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read for existed csv\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./data/dataset_with_sentiment.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format and sort the dataframe by 'Date'\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data_sorted = data.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按 'Date' 和 'sentiment' 分组，然后计算每个类别的 category 为news和opinion的数量\n",
    "category_news_per_day_sentiment = data_sorted[data_sorted['category'] == 'news'].groupby(['Date', 'sentiment']).size().unstack().fillna(0)\n",
    "category_opinion_per_day_sentiment = data_sorted[data_sorted['category'] == 'opinion'].groupby(['Date', 'sentiment']).size().unstack().fillna(0)\n",
    "# 分别计算news和opinion的total\n",
    "category_news_total_per_day_sentiment = data_sorted[data_sorted['category'] == 'news'].groupby(['Date']).size()\n",
    "category_opinion_total_per_day_sentiment = data_sorted[data_sorted['category'] == 'opinion'].groupby(['Date']).size()\n",
    "\n",
    "data_sorted = data_sorted.set_index('Date')\n",
    "data_sorted['P_news_pos'] = category_news_per_day_sentiment['Positive'].reindex(data_sorted.index) / category_news_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_news_neg'] = category_news_per_day_sentiment['Negative'].reindex(data_sorted.index) / category_news_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_op_pos'] = category_opinion_per_day_sentiment['Positive'].reindex(data_sorted.index) / category_opinion_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted['P_op_neg'] = category_opinion_per_day_sentiment['Negative'].reindex(data_sorted.index) / category_opinion_total_per_day_sentiment.reindex(data_sorted.index)\n",
    "data_sorted = data_sorted.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data = data_sorted.groupby('Date').last()\n",
    "\n",
    "# Shift the 'Open' column to get the next day's opening price\n",
    "daily_data['Next_Open'] = daily_data['Open'].shift(-1)\n",
    "\n",
    "# Drop the last row as it will not have a 'Next_Open' value\n",
    "daily_data = daily_data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2016-10-28')\n",
    "daily_data.loc[(date_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2016-10-28')\n",
    "data_sorted.loc[data_sorted['Date'] == date_to_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2020-01-23')\n",
    "daily_data.loc[(date_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_query = pd.to_datetime('2018-05-06')\n",
    "category_to_query = 'news'\n",
    "data_sorted.loc[(data_sorted['Date'] == date_to_query) & (data_sorted['category'] == category_to_query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data['P_news_neg'].fillna(0, inplace=True)\n",
    "daily_data['P_news_pos'].fillna(0, inplace=True)\n",
    "daily_data['P_op_neg'].fillna(0, inplace=True)\n",
    "daily_data['P_op_pos'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the dataset into a Pandas DataFrame\n",
    "historical_data = pd.read_csv('data/AAPL_Yahoo_Correct.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "\n",
    "# Plotting the 'Open' price against the 'Date'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(historical_data['Date'], historical_data['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Date将historical_data的全部列和daily_data的这四个P_开头的列合并。如果出现有些天在daily_data中不存在，则四个P_开头的列在这一天都置为0。\n",
    "daily_data_merged = pd.merge(historical_data, daily_data[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']], left_on='Date', right_index=True, how='left')\n",
    "daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']] = daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(daily_data_merged['Date'], daily_data_merged['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.to_csv('./data/dataset_FinBERT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.1: VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the dataset into a Pandas DataFrame\n",
    "historical_data = pd.read_csv('data/AAPL_Yahoo_Correct.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "historical_data['Date'] = pd.to_datetime(historical_data['Date'])\n",
    "\n",
    "# Plotting the 'Open' price against the 'Date'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(historical_data['Date'], historical_data['Open'], label='AAPL Open Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price ($)')\n",
    "plt.title('AAPL Stock Open Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read VADER results from csv\n",
    "# vader_daily_results = pd.read_csv('./data/VADER_results.csv')\n",
    "# vader_daily_results = pd.read_csv('./data/combined_data_mean5.csv')\n",
    "# vader_daily_results = pd.read_csv('./data/combined_data_mean_first512.csv')\n",
    "vader_daily_results = pd.read_csv('./data/proportion_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_daily_results['Date'] = pd.to_datetime(vader_daily_results['Date'])\n",
    "vader_daily_results = vader_daily_results[['Date', 'news_neg', 'news_pos', 'opinion_neg', 'opinion_pos']]\n",
    "vader_daily_results.columns = ['Date', 'P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']\n",
    "vader_daily_results.set_index('Date', inplace=True)\n",
    "\n",
    "daily_data_merged = pd.merge(historical_data, vader_daily_results[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']], left_on='Date', right_index=True, how='left')\n",
    "daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']] = daily_data_merged[['P_news_neg', 'P_news_pos', 'P_op_neg', 'P_op_pos']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_merged.to_csv('./data/dataset_VADER.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Stock price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# FinBERT\n",
    "daily_data_merged = pd.read_csv('./data/dataset_FinBERT.csv', index_col=0)\n",
    "# VADER\n",
    "# daily_data_merged = pd.read_csv('./data/dataset_VADER.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Showing sentiment analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment_results = daily_data_merged[['Date', 'P_news_pos', 'P_news_neg', 'P_op_pos', 'P_op_neg']]\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the data into two parts: news sentiment analysis results and opinion sentiment analysis results\n",
    "news_sentiment_results = daily_sentiment_results[['Date', 'P_news_pos', 'P_news_neg']]\n",
    "opinion_sentiment_results = daily_sentiment_results[['Date', 'P_op_pos', 'P_op_neg']]\n",
    "\n",
    "# Ensure 'Date' is in news_sentiment_results and opinion_sentiment_results\n",
    "assert 'Date' in news_sentiment_results.columns\n",
    "assert 'Date' in opinion_sentiment_results.columns\n",
    "\n",
    "# Set 'Date' column as index\n",
    "news_sentiment_results.set_index('Date', inplace=True)\n",
    "opinion_sentiment_results.set_index('Date', inplace=True)\n",
    "\n",
    "# Draw a heatmap for news sentiment analysis results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('News Sentiment Analysis Results')\n",
    "sns.heatmap(news_sentiment_results.tail(5), annot=True, cmap='YlGnBu', fmt=\".3f\")\n",
    "plt.show()\n",
    "\n",
    "# Draw a heatmap for opinion sentiment analysis results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Opinion Sentiment Analysis Results')\n",
    "sns.heatmap(opinion_sentiment_results.tail(5), annot=True, cmap='YlGnBu', fmt=\".3f\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择特征和目标\n",
    "# 保留原本的index，将'Date'列单独提取出来保存\n",
    "date = daily_data_merged['Date']\n",
    "date = pd.to_datetime(date)\n",
    "\n",
    "features = daily_data_merged.drop(['Date'], axis=1)\n",
    "# Open作为预测目标\n",
    "target = daily_data_merged['Open']\n",
    "features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply the MinMaxScaler to the features and target\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# fit_transform根据数据计算缩放参数\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# 保存缩放参数\n",
    "import joblib\n",
    "joblib.dump(scaler_features, './model/scaler_features.pkl')\n",
    "joblib.dump(scaler_target, './model/scaler_target.pkl')\n",
    "\n",
    "# Create new DataFrames with the scaled features and target\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "scaled_target_df = pd.DataFrame(scaled_target, columns=['Open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features, targets, seq_length):\n",
    "    \"\"\"\n",
    "    Create sequences of specified length from time series data.\n",
    "\n",
    "    Args:\n",
    "    features (np.array): The feature data.\n",
    "    targets (np.array): The target data.\n",
    "    seq_length (int): The length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Sequences of features.\n",
    "    np.array: Corresponding targets for each sequence.\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        x = features[i:(i + seq_length)]\n",
    "        y = targets[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length\n",
    "seq_length = 25\n",
    "\n",
    "# Create sequences\n",
    "features_seq, target_seq = create_sequences(scaled_features, scaled_target, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "    features_seq, target_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_features, test_features, val_target, test_target = train_test_split(\n",
    "    test_features, test_target, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences to Tensor\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_target = torch.tensor(train_target, dtype=torch.float32)\n",
    "\n",
    "val_features = torch.tensor(val_features, dtype=torch.float32)\n",
    "val_target = torch.tensor(val_target, dtype=torch.float32)\n",
    "\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_target = torch.tensor(test_target, dtype=torch.float32)\n",
    "\n",
    "# 创建TensorDataset\n",
    "train_dataset = TensorDataset(train_features, train_target)\n",
    "val_dataset = TensorDataset(val_features, val_target)\n",
    "test_dataset = TensorDataset(test_features, test_target)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用普通的LSTM模型，不使用注意力机制\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_layers, output_dim, dropout=0.2):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # 打印lstm_out的形状\n",
    "        # print(lstm_out.shape)\n",
    "        # 取最后一个时间步的输出\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用注意力机制的LSTM\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.attn = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         attn_weights = torch.tanh(self.attn(encoder_outputs))\n",
    "#         return torch.bmm(attn_weights.transpose(1, 2), encoder_outputs).squeeze(1)\n",
    "\n",
    "# class AttentionLSTM(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_size, num_layers, output_dim, dropout=0.2):\n",
    "#         super(AttentionLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "\n",
    "#         # LSTM层\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_size, num_layers, \n",
    "#                             batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "#         # 注意力层\n",
    "#         self.attention = Attention(hidden_size)\n",
    "        \n",
    "#         # 全连接层\n",
    "#         self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         attn_out = self.attention(lstm_out[:, -1, :], lstm_out)\n",
    "#         output = self.fc(attn_out)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "input_dim = scaled_features_df.shape[1]  # 特征数量\n",
    "hidden_size = 100  # 隐藏状态中的特征数量，可以调整\n",
    "num_layers = 4    # 堆叠的LSTM层的数量\n",
    "output_dim = 1    # 输出维度的数量（预测一个值）\n",
    "\n",
    "# 使用SimpleLSTM\n",
    "model = SimpleLSTM(input_dim, hidden_size, num_layers, output_dim, dropout=0.2)\n",
    "# 使用AttentionLSTM\n",
    "# model = AttentionLSTM(input_dim, hidden_size, num_layers, output_dim, dropout=0.2)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './model/best_model.pth')\n",
    "        print('best_model updated at epoch {}, best_val_loss : {:.4f}'.format(epoch+1, best_val_loss))\n",
    "        \n",
    "    # 每5轮打印一次train loss和val loss\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # 记录两个loss\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "# 在所有epochs结束后绘制损失图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_epochs), train_loss_list, label='Train Loss', color='blue')\n",
    "plt.plot(range(num_epochs), val_loss_list, label='Validation Loss', color='red')\n",
    "plt.title('Train Loss and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load('./model/best_model.pth'))\n",
    "\n",
    "# 计算新的测试集的大小\n",
    "test_size_new = int(len(features_seq) * 0.05)\n",
    "\n",
    "# 按时间顺序划分新的测试集\n",
    "test_features_new, test_target_new = features_seq[-test_size_new:], target_seq[-test_size_new:]\n",
    "\n",
    "# 使用模型进行预测\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions_new = model(torch.tensor(test_features_new, dtype=torch.float32)).numpy()\n",
    "\n",
    "# 反缩放预测值\n",
    "test_predictions_new = scaler_target.inverse_transform(test_predictions_new).flatten()\n",
    "\n",
    "# 反缩放真实目标值\n",
    "test_target_new = scaler_target.inverse_transform(test_target_new.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 计算日期的总长度\n",
    "total_length = len(date)\n",
    "\n",
    "# 计算测试集的开始位置\n",
    "test_start = total_length - test_size_new\n",
    "\n",
    "# 计算新的测试集的结束位置\n",
    "test_end = total_length\n",
    "\n",
    "# 获取新的测试集的日期范围\n",
    "test_date_new = date[test_start:test_end]\n",
    "\n",
    "# Print the date range of the new test set\n",
    "print(\"The date range of the new test set is from\", test_date_new.iloc[0], \"to\", test_date_new.iloc[-1])\n",
    "\n",
    "# Print the length of the new test set\n",
    "print(\"The length is\", len(test_target_new))\n",
    "\n",
    "# 绘制实际股价和预测股价的对比图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(date[test_start:], test_target_new, label='Actual Prices', color='blue')\n",
    "plt.plot(date[test_start:], test_predictions_new, label='Predicted Prices', color='red')\n",
    "\n",
    "# 设置x轴的日期格式\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=10))  # 设置日期间隔\n",
    "\n",
    "plt.title('Predicted vs Actual Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.gcf().autofmt_xdate()  # 自动调整x轴日期标签的角度以提高可读性\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 计算MSE\n",
    "mse = mean_squared_error(test_target_new, test_predictions_new)\n",
    "print('Test MSE: ', mse)\n",
    "# 计算RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print('Test RMSE: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 50个epoch RMSE测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建一个数据框来显示序列长度和对应的RMSE\n",
    "seq_rmse_df = pd.DataFrame({\n",
    "    'Seq_length': [10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'RMSE': [1.1130488933158105, 0.8456300018583505, 0.8641183126757913, 0.8094936259619585, 0.9580832258849209, 0.8409732222581049, 0.9262937021912081, 1.1064503987204368, 0.9005207820679706]\n",
    "})\n",
    "\n",
    "# 将RMSE保留两位小数\n",
    "seq_rmse_df['RMSE'] = seq_rmse_df['RMSE'].round(3)\n",
    "seq_rmse_df = seq_rmse_df.reset_index(drop=True)\n",
    "\n",
    "import matplotlib.cm\n",
    "from plottable import ColumnDefinition, Table\n",
    "\n",
    "# 创建列定义\n",
    "seq_length_col_def = ColumnDefinition('Seq_length', title='sequence length')\n",
    "rmse_col_def = ColumnDefinition('RMSE', title='RMSE', cmap=matplotlib.cm.get_cmap('viridis'))\n",
    "\n",
    "# 创建表格\n",
    "table = Table(seq_rmse_df, column_definitions=[seq_length_col_def, rmse_col_def])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "rmse_results = [1.1130488933158105, 0.8456300018583505, 0.8641183126757913, 0.8094936259619585, 0.9580832258849209, 0.8409732222581049, 0.9262937021912081, 1.1064503987204368, 0.9005207820679706]\n",
    "\n",
    "# 绘制RMSE结果图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, rmse_results, label='RMSE Results', color='blue')\n",
    "\n",
    "# 标出最小RMSE点\n",
    "min_rmse_index = np.argmin(rmse_results)\n",
    "plt.plot(seq_lengths[min_rmse_index], rmse_results[min_rmse_index], 'ro')\n",
    "plt.text(seq_lengths[min_rmse_index], rmse_results[min_rmse_index], f'Min RMSE: {rmse_results[min_rmse_index]:.3f}', fontsize=12, ha='right')\n",
    "\n",
    "plt.title('RMSE Results for Different Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Method': ['Historical transaction information', 'FinBERT mixed', 'VADER mixed'],\n",
    "    'RMSE': [0.9793, 0.7341, 0.8601]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 设置表格样式\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 创建一个新的figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 使用seaborn的barplot函数绘制条形图\n",
    "sns.barplot(x='Method', y='RMSE', data=df, palette='viridis')\n",
    "\n",
    "# 设置标题和坐标轴标签\n",
    "plt.title('RMSE Results for Different Methods')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
