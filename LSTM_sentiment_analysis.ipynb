{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"./data/NEWS_YAHOO_stock_prediction.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "texts = df['content'].astype(str)\n",
    "\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "maxlen = 200\n",
    "df_padded = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.random.randint(2, size=(len(texts),))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_padded, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=maxlen))\n",
    "model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "all_sequences = tokenizer.texts_to_sequences(texts)\n",
    "all_df_padded = pad_sequences(all_sequences, maxlen=maxlen)\n",
    "predictions = model.predict(all_df_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_counts = []\n",
    "negative_counts = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    text = texts[i]\n",
    "    sentences = text.split('.')\n",
    "    sequence = tokenizer.texts_to_sequences(sentences)\n",
    "    padded = pad_sequences(sequence, maxlen=maxlen)\n",
    "    sentence_predictions = model.predict(padded)\n",
    "\n",
    "    pos_count = np.sum(sentence_predictions >= 0.7)\n",
    "    neg_count = np.sum(sentence_predictions < 0.3)\n",
    "\n",
    "    positive_counts.append(pos_count)\n",
    "    negative_counts.append(neg_count)\n",
    "\n",
    "article_sentiments = ['Positive' if pos_count > neg_count else 'Negative' for pos_count, neg_count in\n",
    "                      zip(positive_counts, negative_counts)]\n",
    "\n",
    "N_total = len(texts)\n",
    "N_pos = sum(1 for sentiment in article_sentiments if sentiment == 'Positive')\n",
    "N_neg = N_total - N_pos\n",
    "\n",
    "P_pos = N_pos / N_total\n",
    "P_neg = N_neg / N_total\n",
    "\n",
    "print(f\"P_pos: {P_pos:.2f}\")\n",
    "print(f\"P_neg: {P_neg:.2f}\")\n",
    "\n",
    "for text, sentiment in zip(texts[:10], article_sentiments[:10]):\n",
    "    print(f\"{text[:50]}... -> Overall Sentiment: {sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
